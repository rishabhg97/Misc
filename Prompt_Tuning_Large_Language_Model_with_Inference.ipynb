{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhg97/Misc/blob/main/Prompt_Tuning_Large_Language_Model_with_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab file is created by [Pragnakalp Techlabs](https://pragnakalp.com/) using [Nvidia Nemo](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Multitask_Prompt_and_PTuning.ipynb).\n",
        "\n",
        "You can copy this colab in your drive and then execute the command in given order. For more details check our [blog](https://www.pragnakalp.com/prompt-tuning-for-large-language-models-with-inference/)."
      ],
      "metadata": {
        "id": "ZctgsYE5MkoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This colab file is to prompt tune GPT 1.3B for Sentiment Analysis task.**"
      ],
      "metadata": {
        "id": "kJ7zOobXA9F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NeMo Installation**\n",
        "\n",
        "Setup notebook:\n",
        "\n",
        "\n",
        "1.   Open a new Python 3 notebook.\n",
        "\n",
        "2.   Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "\n",
        "3. Run the cell below to set up dependencie\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gormg0BykQyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BRANCH='main'"
      ],
      "metadata": {
        "id": "CFx6SaYKlisP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhO25x_QDru1",
        "outputId": "da18229e-9b09-4e56-bbb7-6c5ee5592d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,235 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,143 kB]\n",
            "Fetched 3,659 kB in 6s (622 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.28-4ubuntu0.18.04.2).\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7iF9LvCDr_a",
        "outputId": "5975f3a0-f70a-4e99-ce06-bb20927a6636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (0.29.32)\n"
          ]
        }
      ],
      "source": [
        "!pip install Cython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==1.7.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muvz581P3YVB",
        "outputId": "68b1ffdf-ad91-4cde-8ab4-bbef8d9229b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning==1.7.7 in /usr/local/lib/python3.8/dist-packages (1.7.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (4.64.1)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (0.3.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (2.9.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (0.10.3)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (2022.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (1.13.0+cu116)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.23.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (6.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.7.7) (3.0.9)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.8.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.19.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (59.5.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.51.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx==1.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ij7mWI63cAZ",
        "outputId": "836118e2-035f-44bc-c8e0-602ad1ae4c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx==1.7.0 in /usr/local/lib/python3.8/dist-packages (1.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo-toolkit==1.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e86oTP12zIg",
        "outputId": "774edb8b-a0f9-4db7-a228-ef9e0a3521e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nemo-toolkit==1.12.0 in /usr/local/lib/python3.8/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (4.64.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.56.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (3.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.3.6)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.21.6)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.17.21)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (59.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.7.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.11.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (2.8.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.14.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (2.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (3.19.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (5.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->nemo-toolkit==1.12.0) (3.0.9)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->nemo-toolkit==1.12.0) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->nemo-toolkit==1.12.0) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->nemo-toolkit==1.12.0) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (3.0.4)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml->nemo-toolkit==1.12.0) (0.2.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqWofZ5BDwzx",
        "outputId": "31c929ff-7c6e-4948-9bb7-62174afb6bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10648, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 10648 (delta 67), reused 59 (delta 33), pack-reused 10523\u001b[K\n",
            "Receiving objects: 100% (10648/10648), 15.15 MiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (7330/7330), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ericharper/apex.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBeHoclhEjw9",
        "outputId": "72759775-2bbb-49a5-d05c-72eba3e48d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/apex\n"
          ]
        }
      ],
      "source": [
        "cd apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qofuUJCAElLK",
        "outputId": "448d32ea-767d-43ec-85db-98d4ea2310fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: checking out 'nm_v1.11.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at e8e5761 comment check\n"
          ]
        }
      ],
      "source": [
        "!git checkout nm_v1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_o9NL7OEmbp",
        "outputId": "58e5cb71-5911-4418-df55-a50efd4ca444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-hibvuvhl\n",
            "Created temporary directory: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Initialized build tracking at /tmp/pip-req-tracker-ajk_2xwp\n",
            "Created build tracker: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Entered build tracker: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Created temporary directory: /tmp/pip-install-2sni3oow\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-vmvyo5j3\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-ajk_2xwp'\n",
            "    Running setup.py (path:/tmp/pip-req-build-vmvyo5j3/setup.py) egg_info for package from file:///content/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-n5k_3v97\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.13.0+cu116\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-vmvyo5j3 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-ajk_2xwp'\n",
            "Created temporary directory: /tmp/pip-unpack-u_3qixbh\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.8/apex\n",
            "  sysconfig: /usr/include/python3.8/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "  Created temporary directory: /tmp/pip-record-ntu444yn\n",
            "    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-vmvyo5j3/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-vmvyo5j3/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext --fast_layer_norm --distributed_adam --deprecated_fused_adam install --record /tmp/pip-record-ntu444yn/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.13.0+cu116\n",
            "\n",
            "\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "    Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "    Cuda compilation tools, release 11.2, V11.2.152\n",
            "    Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.8\n",
            "    creating build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    creating build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    creating build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.8\n",
            "    creating build/temp.linux-x86_64-3.8/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'distributed_adam_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.8/apex\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=distributed_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=distributed_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/distributed_adam_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.8/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/welford.cu -o build/temp.linux-x86_64-3.8/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/syncbn.o build/temp.linux-x86_64-3.8/csrc/welford.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:157:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:178:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:179:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:180:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:202:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:270:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:271:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:272:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:273:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:274:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:275:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm(at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:313:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:332:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:333:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:353:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor rms_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:390:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:391:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:392:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> rms_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:413:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:414:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:415:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:416:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.8/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:86: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "                                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:67:59: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "                                                               ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:69:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:121:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:124:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/mlp.o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_dense_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/fused_dense.cpp: In function ‘at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:30:63: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                   ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:33:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:35:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable ‘b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:64:69: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "                                                                         ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:68:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias = at::empty({out_features}, input.type());\n",
            "                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:70:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:73:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:75:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable ‘d_b_ptr’ [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:106:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:107:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:108:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:111:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:113:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function ‘std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor)’:\n",
            "    csrc/fused_dense.cpp:149:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "                                                                             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:150:74: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "                                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:151:58: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:152:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias2 = at::empty({out_features}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:153:66: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:154:72: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                            ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:157:55: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:159:50: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro ‘AT_DISPATCH_SWITCH’\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro ‘AT_DISPATCH_SWITCH’\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro ‘AT_PRIVATE_CASE_TYPE_USING_HINT’\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro ‘AT_DISPATCH_CASE’\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro ‘AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF’\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/fused_dense.o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.8/csrc/megatron\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'generic_scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_weight_gradient_mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_adam_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function ‘void strided_check_finite(at::Tensor&, at::Tensor&, int, int)’:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:26:2: note: in expansion of macro ‘CHECK_INPUT’\n",
            "      CHECK_INPUT(p_copy);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function ‘void adam(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float)’:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:30:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:31:33: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             if (p_copy.numel() > 0) CHECK_INPUT(p_copy);\n",
            "                                     ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:32:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:33:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:34:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function ‘void reversible_adam(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float)’:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:44:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:45:33: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             if (p_copy.numel() > 0) CHECK_INPUT(p_copy);\n",
            "                                     ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:46:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:47:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:48:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function ‘void maybe_adam_undo(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float)’:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:58:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:59:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:60:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:61:9: note: in expansion of macro ‘CHECK_INPUT’\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function ‘void maybe_cast(at::Tensor&, at::Tensor&, at::Tensor&)’:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:70:2: note: in expansion of macro ‘CHECK_INPUT’\n",
            "      CHECK_INPUT(p_in);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro ‘C10_EXPAND_MSVC_WORKAROUND’\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro ‘TORCH_INTERNAL_ASSERT’\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro ‘AT_ASSERTM’\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:71:2: note: in expansion of macro ‘CHECK_INPUT’\n",
            "      CHECK_INPUT(p_out);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_adam_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fast_layer_norm' extension\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_api.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_api.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -I./apex/contrib/csrc/layer_norm/ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -I./apex/contrib/csrc/layer_norm/ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_api.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fast_layer_norm.cpython-38-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_adam_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/LARC.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/multiproc.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/distributed.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/_autocast_utils.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/_batchsampler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/parallel_state.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/enums.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/log_util.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/microbatches.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/grad_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/commons.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_bert.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/distributed_test_base.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_transformer_lm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_mixed_precision_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/models.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/cells.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/clip_grad.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/conv_bias_relu.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_memory.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/focal_loss.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/index_mul_2d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_lib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck_module_test.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/halo_exchangers.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/mlp.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/amp.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/opt.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/wrap.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/frontend.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/handle.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_initialize.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__version__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/rnn_compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_amp_state.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fast_layer_norm.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/distributed_adam_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/enums.py to enums.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/models.py to models.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/cells.py to cells.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py to peer_halo_exchange_module_tests.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:41: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      if activation is 'none':\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:43: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      elif activation is 'relu':\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      elif activation is 'sigmoid':\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/amp.py to amp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/opt.py to opt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/handle.py to handle.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/compat.py to compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-ntu444yn/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-ajk_2xwp'\n"
          ]
        }
      ],
      "source": [
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --global-option=\"--fast_layer_norm\" --global-option=\"--distributed_adam\" --global-option=\"--deprecated_fused_adam\" ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RaZWSi6EoIz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget \n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xet61W4kXHJU",
        "outputId": "4de75504-e5f2-42ae-a9bf-6f9478485419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXzqgUXlI_Rg"
      },
      "outputs": [],
      "source": [
        "# You can replace DATA_DIR and NEMO_DIR with your own locations\n",
        "DATA_DIR = \"data\"\n",
        "NEMO_DIR = '.'\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GqRWR-g_I_Ow",
        "outputId": "3c19e341-c844-43ad-8c31-33b7917651f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./prompt_learning_financial_phrase_bank_preprocessing.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# download the preprocessing scripts from GitHub\n",
        "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/scripts/dataset_processing/nlp/financial_phrase_bank/prompt_learning_financial_phrase_bank_preprocessing.py', NEMO_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While downloading the below dataset, if you face \"ERROR 403: Forbidden.\" Error, then copy the dataset link and paste in your browser, it will automatically download the zip file. Then you can upload it the colab."
      ],
      "metadata": {
        "id": "k0Mm8txXPAhj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noh20ZCjI_L3",
        "outputId": "ed5bf6d3-9646-4dc0-ce69-cbec126f3ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-30 13:30:44--  https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip\n",
            "Resolving www.researchgate.net (www.researchgate.net)... 104.17.32.105, 104.17.33.105, 2606:4700::6811:2169, ...\n",
            "Connecting to www.researchgate.net (www.researchgate.net)|104.17.32.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-12-30 13:30:44 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILYVf56oI-_3",
        "outputId": "eb10074c-9168-483e-ad9a-1ed2633bc065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  FinancialPhraseBank-v1.0.zip\n",
            "   creating: data/FinancialPhraseBank-v1.0/\n",
            "  inflating: data/FinancialPhraseBank-v1.0/License.txt  \n",
            "   creating: data/__MACOSX/\n",
            "   creating: data/__MACOSX/FinancialPhraseBank-v1.0/\n",
            "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/README.txt  \n",
            "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip FinancialPhraseBank-v1.0.zip -d {DATA_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud_4yDqtI-3g",
        "outputId": "d15a056d-3bcb-4559-b9d5-52a16e20fa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\r\n",
            "For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .@positive\r\n",
            "In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .@positive\r\n",
            "Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .@positive\r\n"
          ]
        }
      ],
      "source": [
        "# What the financial phrase bank dataset looks like before processing\n",
        "SENTIMENT_DIR = os.path.join(DATA_DIR, \"FinancialPhraseBank-v1.0\")\n",
        "!head -4 $SENTIMENT_DIR/Sentences_AllAgree.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwusuUc7Jw0S",
        "outputId": "aa9319f5-c1ae-4692-b92d-95aa9f295875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl\n",
            "\r  0% 0/1811 [00:00<?, ?it/s]\r100% 1811/1811 [00:00<00:00, 233618.89it/s]\n",
            "Saving val split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
            "\r  0% 0/226 [00:00<?, ?it/s]\r100% 226/226 [00:00<00:00, 208883.36it/s]\n",
            "Saving test split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_test.jsonl\n",
            "\r  0% 0/227 [00:00<?, ?it/s]\r100% 227/227 [00:00<00:00, 248009.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocess financial phrase bank dataset\n",
        "!python $NEMO_DIR/prompt_learning_financial_phrase_bank_preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJmMldCJyGj",
        "outputId": "b7c4656a-9d9d-4abf-ee6b-586fba82a8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"taskname\": \"sentiment\", \"sentence\": \"The new organization consists of two business units : Charging & Messaging and Finance & Administration .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Danske Bank is Denmark 's largest bank with 3.5 million customers .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total value of the deliveries is some EUR65m .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"In future , the plant will focus on the production of flange profiles for wind farm towers .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company said that 80 % of the shares of the holding company will be sold to Meadville Holdings Limited , a Hong Kong listed parent company of the Meadville Group .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"With this appointment Kaupthing Bank aims to further co-ordinate Capital Markets activities within the Group and to improve the overall service to clients .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total value of these two contracts is over EUR 21 million .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company 's board of directors will propose a dividend of EUR 0.95 per share for 2008 at the annual general meeting , scheduled to be held on March 23 , 2009 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finlan 's listed food industry company HKScan Group controlled companies in the Baltics improved revenues by EUR 3.5 mn to EUR 160.4 mn in 2010 from EUR 156.9 mn in the year before .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Due to the rapid decrease in net sales , personnel reductions have been carried out on a wider scale than initially expected .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The actions are expected to deliver annual cost savings of some EUR15-20m .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Operating loss totaled EUR 0.3 mn compared to a profit of EUR 2.2 mn in the corresponding period in 2007 .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the six-month period increased , respectively from EUR0 .4 m and EUR3 .2 m , as compared to the corresponding period in 2005 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Therefore , the company 's 2005 result will remain weaker than that of 2004 .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Operating profit margin increased from 11.2 % to 11.7 % .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This new partnership agreement represents a significant milestone for both parties .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"According to Finnish petrol station chain St1 's managing director Kim Wiio , the company was forced to make purchases with rising prices in the first half of 2008 , and now consumer prices are going down almost daily due to competition .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The center will be built in the Kapuli district of Mantsala beside the Hanko-Mantsala-Porvoo road near the new direct rail link between Lahti and Jarvenpaa .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The machine will have an annual production capacity of 200,000 tonnes of super-calendered magazine paper and other paper grades based on recovered fiber , Stora Enso said .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Supported Nokia phones include : N96 , N95-8GB , N95 , N93-N931 , N92 , N85 , N82 , N81 , N80 , N79 , N78 , N77 , N76 , N75 , N73 , N72 , N71 , E90 , E71 , E70 , E66 , E65 , E62 , E61-E61i , E60 , E51 , E50 , Touch Xpress 5800 , 6220 Classic , 6210 Navigator , 6120 Classic , 6110 Navigator , 5700 , 5500 , 5320XM .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Earnings per share EPS amounted to EUR0 .01 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Diluted earnings per share ( EPS ) rose to EUR 1.05 from EUR 0.64 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finnish financial group Aktia 's operating profit for 2009 increased to EUR 47.0 mn from EUR 6.6 mn in 2008 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Metso 's delivery will include a complete coated board line with related air systems and two winders .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Water Treatment Products In Australia Today , Global Research & Data Services is going to publish a market analysis about the market for chemical water treatment products in Australia .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the six-month period increased , respectively , from EUR13 .8 m and EUR143 .6 m , as compared to the corresponding period in 2007 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the 12-month period increased , respectively from EUR20 .8 m and EUR177 .7 m , as compared to the financial year 2004 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Diluted earnings per share ( EPS ) rose to EUR 3.68 from EUR 0.50 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Other details were not provided .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"From Merisatama to the far corners of the world Asfaltti Osakeyhti+\\u00c2 Lemmink+\\u00f1inen was established in 1910 by a group of master builders in Helsinki as a specialist business and subcontractor .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"25 November 2010 - Finnish paints and coatings company Tikkurila Oyj ( HEL : TIK1V ) said today that Finnish state-owned investment company Solidium Oy sold its 14.7 % stake in the company for a total of EUR98m .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"He wore a black beanie-type cap and a black jacket .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finnish silicon wafer technology company Okmetic Oyj OMX Helsinki : OKM1V reported on Thursday 30 October an operating profit of EUR7 .4 m for January-September 2008 , up from EUR6 .1 m in the corresponding period in 2007 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"He joins Technopolis from KONE where he has held various positions within the Group , most recently as Director of Service Business and Business Development for KONE s Middle Eastern operations .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Depending on the market situation , such projects are sold after 1 to 3 years after completion .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company 's plant in Russia will continue to make tyres for its near markets , while the plant in Nokia in Finland will manufacture tyres for other markets .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"After 1 April 2007 Cencorp will not have any own employees in the territory .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Commenting on the deal , Shane Lennon , SVP of Marketing & Product Development at GyPSii said : ?\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The tests , conducted at Nokia Siemens ' LTE center of competence in Espoo , Finland , follow the company 's production start of LTE-ready Flexi Multiradio Base Stations for the 800 MHz band in April 2010 , and complement earlier tests with Nokia on the 2100 MHz and 2600 MHz bands .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This could be any of us at any time , '' she said .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Profit after taxes for the period was up to EUR0 .9 m , from EUR0 .01 m last year .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Short-term licenses for the games cost as little as $ 3 while purchasing a game outright can cost as much as $ 10 or $ 15 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total number of voting rights is 74,612,523 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This amount will not be included in the pensionable salary .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The operating profit for Grain Trading increased to EUR 2.0 mn from EUR 1.4 mn in 2005 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"They are responsible for their own operations , customer relationships , and the development of these .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Consolidated net sales increased 16 % to reach EUR74 .8 m , while operating profit amounted to EUR0 .9 m compared to a loss of EUR0 .7 m in the prior year period .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Okmetic Board of Directors has also decided on a new share ownership program directed to the company 's top management .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The size of a cider bottle will remain unchanged .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"With this , the company will exit the contract manufacturing service segment .\", \"label\": \" neutral\"}\n"
          ]
        }
      ],
      "source": [
        "# What the financial phrase bank dataset looks like after processing\n",
        "!head -50 $SENTIMENT_DIR/financial_phrase_bank_train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxP7SxtuJzL9"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "CONFIG_DIR = os.path.join(NEMO_DIR, \"conf\")\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "\n",
        "# Download the example config file\n",
        "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/megatron_gpt_prompt_learning_config.yaml', CONFIG_DIR)\n",
        "\n",
        "# Load the example config file so we can start editing it\n",
        "CONFIG_PATH = os.path.join(CONFIG_DIR, \"megatron_gpt_prompt_learning_config.yaml\")\n",
        "config = OmegaConf.load(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99kn6SFJ1Np"
      },
      "outputs": [],
      "source": [
        "config.model.data.train_ds = [f\"{SENTIMENT_DIR}/financial_phrase_bank_train.jsonl\"]\n",
        "config.model.data.validation_ds = [f\"{SENTIMENT_DIR}/financial_phrase_bank_val.jsonl\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Formatting**\n",
        "\n"
      ],
      "metadata": {
        "id": "gM1WVrSsm8zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Vh1ymNJ2hV"
      },
      "outputs": [],
      "source": [
        "config.model.task_templates = [\n",
        "    {\n",
        "        \"taskname\": \"sentiment\",\n",
        "        \"prompt_template\": \"<|VIRTUAL_PROMPT_0|> {sentence} sentiment:{label}\",\n",
        "        \"total_virtual_tokens\": 10,\n",
        "        \"virtual_token_splits\": [10],\n",
        "        \"truncate_field\": None,\n",
        "        \"answer_only_loss\": True,\n",
        "        \"answer_field\": \"label\",\n",
        "    },\n",
        "  \n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each task_template item has 5 fields.\n",
        "\n",
        "\n",
        "*  **prompt_template** is a string showing the model where to place virtual tokens and how to map dataset json fields to where they belong in the model prompt.\n",
        "*  **taskname** refers to the same taskname in the dataset json objects.\n",
        "*  **total_virtual_tokens** specifies the total number of virtual tokens that will be inserted into the model prompt.\n",
        "*  **virtual_token_splits** specifies the number of virtual tokens that belong at each <|VIRTUAL_PROMPT_#|> marker. virtual_token_splits values should add up to total_virtual_tokens. The number of virtual_token_splits should match the number of <|VIRTUAL_PROMPT_#|> markers.\n",
        "* **truncate_field** specifies which field in the data json to truncate if the length of the input exceeds the maximum sequence length of the model. If truncate_field is set to None, examples that are too long are simply dropped from the dataset.\n",
        "* **answer_only_loss** Whether to limit loss calculation to only the answer portion of the prompt during tuning. True Strongly recommended for long prompts, but shorter prompts with single word answers seem to benefit from setting this to False.\n",
        "* **answer_field** The field in the data json corresponding to the answer. The loss will only be calculated on this portion of the prompt if answer_only_loss is True. The answer field must be at the end of the prompt template.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KzI-D7mfnTWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Tasks**"
      ],
      "metadata": {
        "id": "u02DpDjdsQUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuQnn9yCJ301"
      },
      "outputs": [],
      "source": [
        "config.model.existing_tasks = []\n",
        "config.model.new_tasks = [\"sentiment\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting The Pre-Trained GPT Model**"
      ],
      "metadata": {
        "id": "-Kmo_DHBslcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We still need to set which GPT model we want to prompt tune. We need .nemo version of LLM model. We have tested neno-megatron-gpt-345m and nemo-megatron-gpt-1.3B on Google Colab Pro.\n",
        "You can download .nemo file from the below link:\n",
        "\n",
        "1. neno-megatron-gpt-345m: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/megatron_gpt_345m.nemo\n",
        "2. nemo-megatron-gpt-1.3B: https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo\n",
        "3. nemo-megatron-mt5-3B: https://huggingface.co/nvidia/nemo-megatron-mt5-3B/resolve/main/nemo_megatron_mt5_3b_bf16_tp1.nemo\n",
        "4. nemo-megatron-t5-3B: https://huggingface.co/nvidia/nemo-megatron-t5-3B/resolve/main/nemo_megatron_t5_3b_bf16_tp1.nemo\n",
        "5. nemo-megatron-gpt-5B: https://huggingface.co/nvidia/nemo-megatron-gpt-5B/resolve/main/nemo_gpt5B_fp16_tp1.nemo\n",
        "6. nemo-megatron-gpt-20B: https://huggingface.co/nvidia/nemo-megatron-gpt-20B/resolve/main/nemo_gpt20B_bf16_tp4.nemo\n",
        "\n"
      ],
      "metadata": {
        "id": "zJ7W85clsyHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Idk1bz-J5TK",
        "outputId": "5bc2dd59-2796-4e39-902e-5afe8630a4fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PretrainedModelInfo(\n",
              " \tpretrained_model_name=megatron_gpt_345m,\n",
              " \tdescription=345M parameter GPT generative Megatron model.,\n",
              " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/megatron_gpt_345m.nemo\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Check the GPT .nemo models that are available\n",
        "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
        "MegatronGPTModel.list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we are using nemo_gpt1.3B_fp16.nemo large model for prompt tuning. You can download the nemo_gpt1.3B_fp16.nemo as shown in below:\n"
      ],
      "metadata": {
        "id": "V7AQ31XltKLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJk_P0DZtulv",
        "outputId": "ab8193b4-b2cc-4cb8-e4d6-2959dd424293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-30 12:56:01--  https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo\n",
            "Resolving huggingface.co (huggingface.co)... 18.235.116.140, 34.238.87.3, 54.144.222.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.235.116.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/67/62/676214ea0cb0c2e59b6a0a42b68a8d0dbbc5082d3db7328c793f08fab0c0d577/0e3c4482d75ecdf622c567690d1323b363bf41074e00b8f7d1a43305535557c3?response-content-disposition=attachment%3B%20filename%3D%22nemo_gpt1.3B_fp16.nemo%22&Expires=1672664162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY3LzYyLzY3NjIxNGVhMGNiMGMyZTU5YjZhMGE0MmI2OGE4ZDBkYmJjNTA4MmQzZGI3MzI4Yzc5M2YwOGZhYjBjMGQ1NzcvMGUzYzQ0ODJkNzVlY2RmNjIyYzU2NzY5MGQxMzIzYjM2M2JmNDEwNzRlMDBiOGY3ZDFhNDMzMDU1MzU1NTdjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMm5lbW9fZ3B0MS4zQl9mcDE2Lm5lbW8lMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzI2NjQxNjJ9fX1dfQ__&Signature=lO7gAiCRXQysiMkUCk2MA1W2kTTf12Sdjmh5WKOvj0tD~CZrD2q7DEI1~eAjlc4hdQOAl5HwsABf~qRgMX2uYtQRgTw6ZddBCeQrUICcus6hcXLUFW-DR5KolvA3uNk5bsYNmGKNVEe5G4ojIRmJQ59p1H62lsm5-rvkRAsF58Khxgb0gMYbJ9UdFJjVRrxCWW3OJpbkwj7MUtWDZSItBnfAK69gpYSAuSDVCoIakrBc0UWeKV6ARAH0X2Bj61NIJHj5vXMoHJSHvIYU1FU9eosB8J5NTDhUwUpfGdl9NoNDVXAt794jMCCz9Fm8YdNEipL6WXyyy~I3OYvGP3200Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-12-30 12:56:02--  https://cdn-lfs.huggingface.co/repos/67/62/676214ea0cb0c2e59b6a0a42b68a8d0dbbc5082d3db7328c793f08fab0c0d577/0e3c4482d75ecdf622c567690d1323b363bf41074e00b8f7d1a43305535557c3?response-content-disposition=attachment%3B%20filename%3D%22nemo_gpt1.3B_fp16.nemo%22&Expires=1672664162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY3LzYyLzY3NjIxNGVhMGNiMGMyZTU5YjZhMGE0MmI2OGE4ZDBkYmJjNTA4MmQzZGI3MzI4Yzc5M2YwOGZhYjBjMGQ1NzcvMGUzYzQ0ODJkNzVlY2RmNjIyYzU2NzY5MGQxMzIzYjM2M2JmNDEwNzRlMDBiOGY3ZDFhNDMzMDU1MzU1NTdjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMm5lbW9fZ3B0MS4zQl9mcDE2Lm5lbW8lMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzI2NjQxNjJ9fX1dfQ__&Signature=lO7gAiCRXQysiMkUCk2MA1W2kTTf12Sdjmh5WKOvj0tD~CZrD2q7DEI1~eAjlc4hdQOAl5HwsABf~qRgMX2uYtQRgTw6ZddBCeQrUICcus6hcXLUFW-DR5KolvA3uNk5bsYNmGKNVEe5G4ojIRmJQ59p1H62lsm5-rvkRAsF58Khxgb0gMYbJ9UdFJjVRrxCWW3OJpbkwj7MUtWDZSItBnfAK69gpYSAuSDVCoIakrBc0UWeKV6ARAH0X2Bj61NIJHj5vXMoHJSHvIYU1FU9eosB8J5NTDhUwUpfGdl9NoNDVXAt794jMCCz9Fm8YdNEipL6WXyyy~I3OYvGP3200Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.24.38, 13.35.24.76, 13.35.24.128, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.24.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3017949806 (2.8G) [application/x-gzip]\n",
            "Saving to: ‘nemo_gpt1.3B_fp16.nemo’\n",
            "\n",
            "nemo_gpt1.3B_fp16.n 100%[===================>]   2.81G  21.4MB/s    in 2m 25s  \n",
            "\n",
            "2022-12-30 12:58:28 (19.8 MB/s) - ‘nemo_gpt1.3B_fp16.nemo’ saved [3017949806/3017949806]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ydXg5TEUJ6e"
      },
      "outputs": [],
      "source": [
        "# Set GPT model path on prompt learning config\n",
        "gpt_file_name = \"nemo_gpt1.3B_fp16.nemo\"\n",
        "\n",
        "config.model.language_model_path = gpt_file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also set where we want the final prompt tuned model to be saved by setting model.nemo_path"
      ],
      "metadata": {
        "id": "WaQxGGI7tjSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIOFOrMuUfAp"
      },
      "outputs": [],
      "source": [
        "config.model.nemo_path = \"prompt_tuned_model.nemo\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Prompt-tuning Hyperparameters**"
      ],
      "metadata": {
        "id": "GsS8Fseyt0a2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdjsLJEUhUT"
      },
      "outputs": [],
      "source": [
        "from nemo.collections.nlp.modules.common import VirtualPromptStyle\n",
        "config.model.virtual_prompt_style = VirtualPromptStyle.P_TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeujNl9hUksm"
      },
      "outputs": [],
      "source": [
        "config.model.p_tuning.dropout = 0.0\n",
        "config.model.p_tuning.num_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsPhaMunUl-L",
        "outputId": "41379178-f029-4695-e1c3-d30fe1b44218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 1234\n",
            "nemo_path: prompt_tuned_model.nemo\n",
            "virtual_prompt_style: P_TUNING\n",
            "tensor_model_parallel_size: 1\n",
            "pipeline_model_parallel_size: 1\n",
            "global_batch_size: 8\n",
            "micro_batch_size: 4\n",
            "restore_path: null\n",
            "language_model_path: nemo_gpt1.3B_fp16.nemo\n",
            "save_nemo_on_validation_end: true\n",
            "existing_tasks: []\n",
            "new_tasks:\n",
            "- sentiment\n",
            "sequence_parallel: false\n",
            "activations_checkpoint_granularity: null\n",
            "activations_checkpoint_method: null\n",
            "activations_checkpoint_num_layers: null\n",
            "task_templates:\n",
            "- taskname: sentiment\n",
            "  prompt_template: <|VIRTUAL_PROMPT_0|> {sentence} sentiment:{label}\n",
            "  total_virtual_tokens: 10\n",
            "  virtual_token_splits:\n",
            "  - 10\n",
            "  truncate_field: null\n",
            "  answer_only_loss: true\n",
            "  answer_field: label\n",
            "prompt_tuning:\n",
            "  new_prompt_init_methods:\n",
            "  - text\n",
            "  new_prompt_init_text:\n",
            "  - some init text goes here\n",
            "p_tuning:\n",
            "  encoder_type: tpmlp\n",
            "  dropout: 0.0\n",
            "  num_layers: 2\n",
            "  encoder_hidden: 2048\n",
            "  init_std: 0.023\n",
            "data:\n",
            "  train_ds:\n",
            "  - data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl\n",
            "  validation_ds:\n",
            "  - data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
            "  add_eos: true\n",
            "  shuffle: true\n",
            "  num_workers: 8\n",
            "  pin_memory: true\n",
            "  train_cache_data_path: null\n",
            "  validation_cache_data_path: null\n",
            "  test_cache_data_path: null\n",
            "  load_cache: false\n",
            "optim:\n",
            "  name: fused_adam\n",
            "  lr: 0.0001\n",
            "  weight_decay: 0.01\n",
            "  betas:\n",
            "  - 0.9\n",
            "  - 0.98\n",
            "  sched:\n",
            "    name: CosineAnnealing\n",
            "    warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    monitor: val_loss\n",
            "    reduce_on_plateau: false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Final model config\n",
        "print(OmegaConf.to_yaml(config.model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
        "from pytorch_lightning.plugins.environments.torchelastic_environment import TorchElasticEnvironment\n",
        "\n",
        "# lets modify some trainer configs\n",
        "# checks if we have GPU available and uses it\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.accelerator = accelerator\n",
        "config.trainer.devices = 1\n",
        "config.trainer.max_epochs = 1\n",
        "config.trainer.val_check_interval = 1.0\n",
        "\n",
        "# for PyTorch Native AMP set precision=16\n",
        "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
        "\n",
        "# setup cluster environment parameters\"\n",
        "# use torch elastic cluster environment so `create_process_externally` is True\n",
        "# the launcher is set to None. It will not try to spawn new processes.\n",
        "# It won't create the misconfiguration error because of the `interactive session`\n",
        "os.environ[\"LOCAL_RANK\"] = '0'\n",
        "os.environ[\"RANK\"] = '0'\n",
        "os.environ[\"WORLD_SIZE\"] = '1'\n",
        "\n",
        "strategy = NLPDDPStrategy(find_unused_parameters=False,no_ddp_communication_hook=True)\n",
        "plugins = [TorchElasticEnvironment()]\n",
        "trainer = pl.Trainer(plugins= plugins, strategy=strategy, **config.trainer)\n",
        "\n",
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BImtciFEHdDY",
        "outputId": "86f2f31b-8074-42f5-bb9f-cebf16649599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit native Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer config - \n",
            "\n",
            "devices: 1\n",
            "accelerator: gpu\n",
            "num_nodes: 1\n",
            "precision: 16\n",
            "logger: false\n",
            "enable_checkpointing: false\n",
            "replace_sampler_ddp: false\n",
            "max_epochs: 1\n",
            "max_steps: -1\n",
            "log_every_n_steps: 10\n",
            "val_check_interval: 1.0\n",
            "gradient_clip_val: 1.0\n",
            "resume_from_checkpoint: null\n",
            "benchmark: false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the PyTorch Lightning Trainer**\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
        "\n"
      ],
      "metadata": {
        "id": "JS_kA8o9uGMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JuxqSWIUr3T",
        "outputId": "c7fe8a7e-cd44-49c5-a8d3-72850d22826d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:04:36 exp_manager:291] Experiments will be logged at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36\n",
            "[NeMo I 2022-12-30 13:04:36 exp_manager:669] TensorboardLogger has been set up\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:04:36 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2274: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
            "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
            "    \n",
            "[NeMo W 2022-12-30 13:04:36 exp_manager:919] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
            "[NeMo W 2022-12-30 13:04:36 exp_manager:711] Found save_best_model is True and save_nemo_on_train_end is False. Set save_nemo_on_train_end to True to automatically save the best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36\n"
          ]
        }
      ],
      "source": [
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "# Set name of the experiment \n",
        "config.name = 'sentiment_intent_slot_p_tuning'\n",
        "config.exp_manager.resume_if_exists = False\n",
        "\n",
        "# Init the experiment manager and view the exp_dir\n",
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "exp_dir = str(exp_dir)\n",
        "print(exp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NafqBUES_pI8"
      },
      "outputs": [],
      "source": [
        "# Set some of the learning parameters\n",
        "config.model.optim.lr = 1e-4\n",
        "config.model.precision = config.trainer.precision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt tuning the model**"
      ],
      "metadata": {
        "id": "ox4Nn-0tuXh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "a3ddcac41b3f45fd8c672c4a06cf8f6a",
            "45f2234be5a24dfda8105b35e2331e94",
            "e1aa8279090b4006a3c229051a9f2244",
            "aa591eabf7d64d0f98817f21913cc1c3",
            "2d49abefe02d4d53b0260c2bdad000d4",
            "2dc4235ffff44a7faf52fa2fa9440d2b",
            "187b5d5f17884ca4a3efac58e940b1f2",
            "feae736aa3244c62a4555fb69799144d",
            "f048188eb15b44e4ac140258663e323d",
            "2945f9cd0f6e448682ad5f07897e176d",
            "b846bf1ab1f4426f9ffcfebd9b1f242b",
            "1d471c8bbbcf42c1b1ca554a0a24fc86",
            "92a5a4be717b4032af288b27d87bd9e4",
            "dbe2ef4d36174f3992fba258da4e6d30",
            "61060153d32b44c2a176cc441ccbeb21",
            "6c922e862dfc4036b72d5c13d9b23029",
            "11661e5286534824829d5fc813fe0a45",
            "05aae9da1460429cb448313640ff0d42",
            "69abe9f38ad64cadb49987f62392c46d",
            "0ad90c95e2ed4f0cbf93980cb7c2365d",
            "f62758144c1b4fbab5e21da3a525e502",
            "0e785d6eee214829a9667195561e2710",
            "054719fe01ea4983a4c7cbddb92c2981",
            "5dc2cd106e764deb92b5faef195fd780",
            "ca0e66b9346f45b1a7d8bf46b324688e",
            "e59b089725b24432b434fa56e878f757",
            "9c2987e6ca854926b76c7f485aa44667",
            "e106a86ec2154506970285b173d43a3d",
            "afc83f6900394874961788341488bc04",
            "41935e7ea0384045b20153b5c1423550",
            "a145948cd7374d04a9c4150da16fbdf0",
            "f1e41b1e8e9e42298a9ff7a1eddb3034",
            "e4e87afbb3f649deaa1b7ad74d33253d"
          ]
        },
        "id": "tY5gwHDi_sci",
        "outputId": "c9e47816-92d9-4082-e34b-f76406df9490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:04:43 megatron_init:204] Rank 0 has data parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:207] All data parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:208] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:216] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:217] All model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:227] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:231] All tensor model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:232] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:246] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:258] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:264] All pipeline model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:265] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:266] All embedding group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:267] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:204] Rank 0 has data parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:207] All data parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:208] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:216] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:217] All model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:227] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:231] All tensor model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:232] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:246] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:258] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:264] All pipeline model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:265] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:266] All embedding group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:267] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_utils:210] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "[NeMo I 2022-12-30 13:05:35 megatron_utils:210] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "[NeMo I 2022-12-30 13:05:36 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3ddcac41b3f45fd8c672c4a06cf8f6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d471c8bbbcf42c1b1ca554a0a24fc86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "054719fe01ea4983a4c7cbddb92c2981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:47 megatron_base_model:185] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
            "[NeMo I 2022-12-30 13:05:56 nlp_overrides:370] Model MegatronGPTModel was successfully restored from /content/nemo_gpt1.3B_fp16.nemo.\n",
            "[NeMo I 2022-12-30 13:05:56 auto_tokenizer:171] 10 special tokens added, resize your model accordingly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "from nemo.collections.nlp.models.language_modeling.megatron_gpt_prompt_learning_model import MegatronGPTPromptLearningModel\n",
        "\n",
        "model = MegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53f28eb73b3143b4a3c0c9b7af245e9f",
            "0be4d0a77db1456fb4c6fe94f9097ad7",
            "4726f18878b5475899fd06af3c7451af",
            "a8b3401e4f73452cae980693d10fb50c",
            "19e87a1d8c9c48d88cdd5b740146820c",
            "d1a9c2420a024a539e5c986162865ef6",
            "10f8372bd3cc46d1bd8babdf62d59567",
            "08c7bfcedf9f4f95beae78aa9664c4a4",
            "8b9f366c4d0d4b848e8e63b05508dcbe",
            "0035b4f69e6f4c67897e3da1887eebf8",
            "e8863d42e37647b3801a4d87430d7fa6",
            "0492adb32ca14825881ae78b08659a47",
            "e341623bd3a74bbab561619e6185caf4",
            "e521cdd468044fab9d1fcbda0f8fe847",
            "c34df705810446369b16ffed4494622c",
            "e9d256ad0c424b8ebf8a2e202a56c4a6",
            "2f71ed466e6d43c995bf3b713493b746",
            "47093af54e7b40a088bdcbf3108a6e99",
            "1cff8f66eb164b86b401b12f525afe7f",
            "b42f45ab97b2482f88e5c4a66bc8271a",
            "bf73af868d2f40b59ecf0d589bffdd1d",
            "405251b84b7b4c0c8606ba66c1c00905",
            "77b9cebb5a2f4929b16906e6d1ed3418",
            "9f68e146bb05481190f8e56b9349cc1d",
            "0f5493e840054091b563bcb896ce41b5",
            "f1a6d147e98b4e46ac8daf643885f79e",
            "b1c08923025140569f5338aa7a67381d",
            "14312ed950f04098b68c1d7d75ee5bfc",
            "4d20140def2f4b98918a0d8d89cc739f",
            "a91ec5c5731e413ebcdb57197ac28ae7",
            "7b4979acc63e447fb7bd91144ecec1cb",
            "d01392c7100545f2ba20d4b5185b46f6",
            "1b0502209376485c97c2d144f5a4c1ee",
            "13e121ae5afa4c24bbe3528290ee71a8",
            "814fb06fe14d4c0cbf286cc28b13a305",
            "ff21b332f262448b91d952e4f6098029",
            "9a4bd3af142d4c309d59949bfb29657a",
            "51bbcc37f3d940e9aeeabe83e6a52aa6",
            "b85f0ca3ac6f41c0803f7e8ee06ef448",
            "ff35ece87fe344a28220cc55683ce6ac",
            "519f0ddf92854081bfa9a7d43f2b31c9",
            "0139378a76264587b59a1d12f8342adb",
            "42ab197933cd4de2a8b06fb5f8b2ed84",
            "cc8b306f107548a5964bde9f715db563",
            "c22e6059a4ce4c81a853d10b67dd799c",
            "50ce7a1c97fb4911aa6fff99dbaf7135",
            "135e5372c07a4a63ad83cfdc01a88cad",
            "bc782152f196411791fe1e926318c5ec",
            "3b97147b48dd4382be9b0bb41e2dbb0b",
            "93a24dc5f6174d59a8d882bf3e7f7e17",
            "b741c965c286486086ed2029e3a91d74",
            "692d8999486c42babb7f0fea3beb7025",
            "9cc9e7f9e3f04fcd9f8c0455d8329d04",
            "333f3643f70d4d55b7d16918f2227feb",
            "ef289c7641454ea399129ed8bc074b08"
          ]
        },
        "id": "nHmvxin3_suv",
        "outputId": "1f26e8b1-bbdf-4aeb-ca0e-6581dc06a719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:220: LightningDeprecationWarning: The `LightningModule.on_pretrain_routine_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_fit_start` instead.\n",
            "      rank_zero_deprecation(\n",
            "    \n",
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py:37: UserWarning: MASTER_ADDR environment variable is not defined. Set as localhost\n",
            "      rank_zero_warn(\"MASTER_ADDR environment variable is not defined. Set as localhost\")\n",
            "    \n",
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py:45: UserWarning: MASTER_PORT environment variable is not defined. Set as 12910\n",
            "      rank_zero_warn(\"MASTER_PORT environment variable is not defined. Set as 12910\")\n",
            "    \n",
            "INFO:pytorch_lightning.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:56 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53f28eb73b3143b4a3c0c9b7af245e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:05:57 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "      warnings.warn(_create_warning_msg(\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0492adb32ca14825881ae78b08659a47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:58 nlp_overrides:103] Configuring DDP for model parallelism.\n",
            "[NeMo I 2022-12-30 13:05:58 modelPT:597] Optimizer config = FusedAdam (\n",
            "    Parameter Group 0\n",
            "        betas: [0.9, 0.98]\n",
            "        bias_correction: True\n",
            "        eps: 1e-08\n",
            "        lr: 0.0001\n",
            "        weight_decay: 0.01\n",
            "    )\n",
            "[NeMo I 2022-12-30 13:05:58 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fb74c39f220>\" \n",
            "    will be used during training (effective maximum steps = 226) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    max_steps: 226\n",
            "    )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type                   | Params\n",
            "-----------------------------------------------------------\n",
            "0 | frozen_model    | MegatronGPTModel       | 812 M \n",
            "1 | word_embeddings | VocabParallelEmbedding | 103 M \n",
            "2 | prompt_table    | PromptTable            | 0     \n",
            "3 | prompt_encoder  | PromptEncoderMLP       | 8.4 M \n",
            "-----------------------------------------------------------\n",
            "8.4 M     Trainable params\n",
            "812 M     Non-trainable params\n",
            "820 M     Total params\n",
            "1,641.587 Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77b9cebb5a2f4929b16906e6d1ed3418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:03 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "      warning_cache.warn(\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e121ae5afa4c24bbe3528290ee71a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "      warning_cache.warn(\n",
            "    \n",
            "[NeMo W 2022-12-30 13:06:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c22e6059a4ce4c81a853d10b67dd799c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 226: 'val_loss' reached 0.06929 (best 0.06929), saving model to '/content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt' as top 2\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restored all states from the checkpoint file at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:55 megatron_gpt_prompt_learning_model:683] All p-tuned prompts where moved to the prompt table.\n",
            "[NeMo I 2022-12-30 13:06:55 megatron_gpt_prompt_learning_model:696] The final model was saved to prompt_tuned_model.nemo\n"
          ]
        }
      ],
      "source": [
        "# Training set to 10 epochs by default in a cell above\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference after Prompt-tuning**"
      ],
      "metadata": {
        "id": "-57ykzIuuf08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtoDPK5nGbqE"
      },
      "outputs": [],
      "source": [
        "test_examples = [\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The products have a low salt and fat content .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The agreement is valid for four years .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The company is well positioned in Brazil and Uruguay .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The movie was not good.\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375,
          "referenced_widgets": [
            "c696b322c5b34b94a6c2d8880a6b66f7",
            "b45429a13cb047a681b876c33e4f58a6",
            "77b061c305d441d0bc1caf49e4e0216a",
            "e8f808e677ae43049fe3f0d078d50b4a",
            "5b7949c7feb94a7088293772e73c899b",
            "01566330fca842ce9c347d284a870242",
            "23daa37100434f25af5a2450cd553c81",
            "badf53d5820a461faf5acd8788a770af",
            "dc928330eca241b69718f1590d57f37e",
            "0e4d87949f2342c181c749ddb4643ad3",
            "9d671c55e33c4ac29ee49616b8345cdc"
          ]
        },
        "id": "6w145640MKTb",
        "outputId": "683936a7-9a50-4a2b-a3f1-cd3c180b8402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:56 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c696b322c5b34b94a6c2d8880a6b66f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:56 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:57 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
            "      warnings.warn(\"This function is only for unittest\")\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction results of some sample queries with the trained model:\n",
            "The products have a low salt and fat content . sentiment: neutral\n",
            "------------------------------\n",
            "The agreement is valid for four years . sentiment: neutral\n",
            "------------------------------\n",
            "Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
            "------------------------------\n",
            "The company is well positioned in Brazil and Uruguay . sentiment: neutral\n",
            "------------------------------\n",
            "Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
            "------------------------------\n",
            "The movie was not good. sentiment: negative\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "response = model.generate(inputs=test_examples, length_params=None)\n",
        "\n",
        "\n",
        "print('The prediction results of some sample queries with the trained model:')\n",
        "for result in response['sentences']:\n",
        "    print(result)\n",
        "    print(\"-\" * 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3ddcac41b3f45fd8c672c4a06cf8f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45f2234be5a24dfda8105b35e2331e94",
              "IPY_MODEL_e1aa8279090b4006a3c229051a9f2244",
              "IPY_MODEL_aa591eabf7d64d0f98817f21913cc1c3"
            ],
            "layout": "IPY_MODEL_2d49abefe02d4d53b0260c2bdad000d4"
          }
        },
        "45f2234be5a24dfda8105b35e2331e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc4235ffff44a7faf52fa2fa9440d2b",
            "placeholder": "​",
            "style": "IPY_MODEL_187b5d5f17884ca4a3efac58e940b1f2",
            "value": "Downloading config.json: 100%"
          }
        },
        "e1aa8279090b4006a3c229051a9f2244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feae736aa3244c62a4555fb69799144d",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f048188eb15b44e4ac140258663e323d",
            "value": 665
          }
        },
        "aa591eabf7d64d0f98817f21913cc1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2945f9cd0f6e448682ad5f07897e176d",
            "placeholder": "​",
            "style": "IPY_MODEL_b846bf1ab1f4426f9ffcfebd9b1f242b",
            "value": " 665/665 [00:00&lt;00:00, 42.1kB/s]"
          }
        },
        "2d49abefe02d4d53b0260c2bdad000d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc4235ffff44a7faf52fa2fa9440d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187b5d5f17884ca4a3efac58e940b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feae736aa3244c62a4555fb69799144d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f048188eb15b44e4ac140258663e323d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2945f9cd0f6e448682ad5f07897e176d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b846bf1ab1f4426f9ffcfebd9b1f242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d471c8bbbcf42c1b1ca554a0a24fc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92a5a4be717b4032af288b27d87bd9e4",
              "IPY_MODEL_dbe2ef4d36174f3992fba258da4e6d30",
              "IPY_MODEL_61060153d32b44c2a176cc441ccbeb21"
            ],
            "layout": "IPY_MODEL_6c922e862dfc4036b72d5c13d9b23029"
          }
        },
        "92a5a4be717b4032af288b27d87bd9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11661e5286534824829d5fc813fe0a45",
            "placeholder": "​",
            "style": "IPY_MODEL_05aae9da1460429cb448313640ff0d42",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "dbe2ef4d36174f3992fba258da4e6d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69abe9f38ad64cadb49987f62392c46d",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ad90c95e2ed4f0cbf93980cb7c2365d",
            "value": 1042301
          }
        },
        "61060153d32b44c2a176cc441ccbeb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62758144c1b4fbab5e21da3a525e502",
            "placeholder": "​",
            "style": "IPY_MODEL_0e785d6eee214829a9667195561e2710",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 1.04MB/s]"
          }
        },
        "6c922e862dfc4036b72d5c13d9b23029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11661e5286534824829d5fc813fe0a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05aae9da1460429cb448313640ff0d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69abe9f38ad64cadb49987f62392c46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad90c95e2ed4f0cbf93980cb7c2365d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f62758144c1b4fbab5e21da3a525e502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e785d6eee214829a9667195561e2710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054719fe01ea4983a4c7cbddb92c2981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc2cd106e764deb92b5faef195fd780",
              "IPY_MODEL_ca0e66b9346f45b1a7d8bf46b324688e",
              "IPY_MODEL_e59b089725b24432b434fa56e878f757"
            ],
            "layout": "IPY_MODEL_9c2987e6ca854926b76c7f485aa44667"
          }
        },
        "5dc2cd106e764deb92b5faef195fd780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e106a86ec2154506970285b173d43a3d",
            "placeholder": "​",
            "style": "IPY_MODEL_afc83f6900394874961788341488bc04",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "ca0e66b9346f45b1a7d8bf46b324688e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41935e7ea0384045b20153b5c1423550",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a145948cd7374d04a9c4150da16fbdf0",
            "value": 456318
          }
        },
        "e59b089725b24432b434fa56e878f757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e41b1e8e9e42298a9ff7a1eddb3034",
            "placeholder": "​",
            "style": "IPY_MODEL_e4e87afbb3f649deaa1b7ad74d33253d",
            "value": " 446k/446k [00:00&lt;00:00, 500kB/s]"
          }
        },
        "9c2987e6ca854926b76c7f485aa44667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e106a86ec2154506970285b173d43a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc83f6900394874961788341488bc04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41935e7ea0384045b20153b5c1423550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a145948cd7374d04a9c4150da16fbdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1e41b1e8e9e42298a9ff7a1eddb3034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e87afbb3f649deaa1b7ad74d33253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53f28eb73b3143b4a3c0c9b7af245e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be4d0a77db1456fb4c6fe94f9097ad7",
              "IPY_MODEL_4726f18878b5475899fd06af3c7451af",
              "IPY_MODEL_a8b3401e4f73452cae980693d10fb50c"
            ],
            "layout": "IPY_MODEL_19e87a1d8c9c48d88cdd5b740146820c"
          }
        },
        "0be4d0a77db1456fb4c6fe94f9097ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a9c2420a024a539e5c986162865ef6",
            "placeholder": "​",
            "style": "IPY_MODEL_10f8372bd3cc46d1bd8babdf62d59567",
            "value": ""
          }
        },
        "4726f18878b5475899fd06af3c7451af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c7bfcedf9f4f95beae78aa9664c4a4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b9f366c4d0d4b848e8e63b05508dcbe",
            "value": 1
          }
        },
        "a8b3401e4f73452cae980693d10fb50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0035b4f69e6f4c67897e3da1887eebf8",
            "placeholder": "​",
            "style": "IPY_MODEL_e8863d42e37647b3801a4d87430d7fa6",
            "value": " 1811/? [00:01&lt;00:00, 1804.91it/s]"
          }
        },
        "19e87a1d8c9c48d88cdd5b740146820c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a9c2420a024a539e5c986162865ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f8372bd3cc46d1bd8babdf62d59567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c7bfcedf9f4f95beae78aa9664c4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8b9f366c4d0d4b848e8e63b05508dcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0035b4f69e6f4c67897e3da1887eebf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8863d42e37647b3801a4d87430d7fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0492adb32ca14825881ae78b08659a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e341623bd3a74bbab561619e6185caf4",
              "IPY_MODEL_e521cdd468044fab9d1fcbda0f8fe847",
              "IPY_MODEL_c34df705810446369b16ffed4494622c"
            ],
            "layout": "IPY_MODEL_e9d256ad0c424b8ebf8a2e202a56c4a6"
          }
        },
        "e341623bd3a74bbab561619e6185caf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f71ed466e6d43c995bf3b713493b746",
            "placeholder": "​",
            "style": "IPY_MODEL_47093af54e7b40a088bdcbf3108a6e99",
            "value": ""
          }
        },
        "e521cdd468044fab9d1fcbda0f8fe847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cff8f66eb164b86b401b12f525afe7f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b42f45ab97b2482f88e5c4a66bc8271a",
            "value": 1
          }
        },
        "c34df705810446369b16ffed4494622c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf73af868d2f40b59ecf0d589bffdd1d",
            "placeholder": "​",
            "style": "IPY_MODEL_405251b84b7b4c0c8606ba66c1c00905",
            "value": " 226/? [00:00&lt;00:00, 1447.11it/s]"
          }
        },
        "e9d256ad0c424b8ebf8a2e202a56c4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f71ed466e6d43c995bf3b713493b746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47093af54e7b40a088bdcbf3108a6e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cff8f66eb164b86b401b12f525afe7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b42f45ab97b2482f88e5c4a66bc8271a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf73af868d2f40b59ecf0d589bffdd1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "405251b84b7b4c0c8606ba66c1c00905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b9cebb5a2f4929b16906e6d1ed3418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f68e146bb05481190f8e56b9349cc1d",
              "IPY_MODEL_0f5493e840054091b563bcb896ce41b5",
              "IPY_MODEL_f1a6d147e98b4e46ac8daf643885f79e"
            ],
            "layout": "IPY_MODEL_b1c08923025140569f5338aa7a67381d"
          }
        },
        "9f68e146bb05481190f8e56b9349cc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14312ed950f04098b68c1d7d75ee5bfc",
            "placeholder": "​",
            "style": "IPY_MODEL_4d20140def2f4b98918a0d8d89cc739f",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "0f5493e840054091b563bcb896ce41b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91ec5c5731e413ebcdb57197ac28ae7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b4979acc63e447fb7bd91144ecec1cb",
            "value": 2
          }
        },
        "f1a6d147e98b4e46ac8daf643885f79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01392c7100545f2ba20d4b5185b46f6",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0502209376485c97c2d144f5a4c1ee",
            "value": " 2/2 [00:05&lt;00:00,  2.60s/it]"
          }
        },
        "b1c08923025140569f5338aa7a67381d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "14312ed950f04098b68c1d7d75ee5bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d20140def2f4b98918a0d8d89cc739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a91ec5c5731e413ebcdb57197ac28ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4979acc63e447fb7bd91144ecec1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d01392c7100545f2ba20d4b5185b46f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0502209376485c97c2d144f5a4c1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e121ae5afa4c24bbe3528290ee71a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_814fb06fe14d4c0cbf286cc28b13a305",
              "IPY_MODEL_ff21b332f262448b91d952e4f6098029",
              "IPY_MODEL_9a4bd3af142d4c309d59949bfb29657a"
            ],
            "layout": "IPY_MODEL_51bbcc37f3d940e9aeeabe83e6a52aa6"
          }
        },
        "814fb06fe14d4c0cbf286cc28b13a305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85f0ca3ac6f41c0803f7e8ee06ef448",
            "placeholder": "​",
            "style": "IPY_MODEL_ff35ece87fe344a28220cc55683ce6ac",
            "value": "Epoch 0: 100%"
          }
        },
        "ff21b332f262448b91d952e4f6098029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519f0ddf92854081bfa9a7d43f2b31c9",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0139378a76264587b59a1d12f8342adb",
            "value": 254
          }
        },
        "9a4bd3af142d4c309d59949bfb29657a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ab197933cd4de2a8b06fb5f8b2ed84",
            "placeholder": "​",
            "style": "IPY_MODEL_cc8b306f107548a5964bde9f715db563",
            "value": " 254/254 [00:51&lt;00:00,  4.98it/s, loss=0.151, v_num=4-36, reduced_train_loss=0.0472, global_step=225.0, val_loss=0.0693]"
          }
        },
        "51bbcc37f3d940e9aeeabe83e6a52aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b85f0ca3ac6f41c0803f7e8ee06ef448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff35ece87fe344a28220cc55683ce6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "519f0ddf92854081bfa9a7d43f2b31c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0139378a76264587b59a1d12f8342adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42ab197933cd4de2a8b06fb5f8b2ed84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8b306f107548a5964bde9f715db563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c22e6059a4ce4c81a853d10b67dd799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50ce7a1c97fb4911aa6fff99dbaf7135",
              "IPY_MODEL_135e5372c07a4a63ad83cfdc01a88cad",
              "IPY_MODEL_bc782152f196411791fe1e926318c5ec"
            ],
            "layout": "IPY_MODEL_3b97147b48dd4382be9b0bb41e2dbb0b"
          }
        },
        "50ce7a1c97fb4911aa6fff99dbaf7135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93a24dc5f6174d59a8d882bf3e7f7e17",
            "placeholder": "​",
            "style": "IPY_MODEL_b741c965c286486086ed2029e3a91d74",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "135e5372c07a4a63ad83cfdc01a88cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_692d8999486c42babb7f0fea3beb7025",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cc9e7f9e3f04fcd9f8c0455d8329d04",
            "value": 28
          }
        },
        "bc782152f196411791fe1e926318c5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_333f3643f70d4d55b7d16918f2227feb",
            "placeholder": "​",
            "style": "IPY_MODEL_ef289c7641454ea399129ed8bc074b08",
            "value": " 28/28 [00:02&lt;00:00, 10.12it/s]"
          }
        },
        "3b97147b48dd4382be9b0bb41e2dbb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "93a24dc5f6174d59a8d882bf3e7f7e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b741c965c286486086ed2029e3a91d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "692d8999486c42babb7f0fea3beb7025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc9e7f9e3f04fcd9f8c0455d8329d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333f3643f70d4d55b7d16918f2227feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef289c7641454ea399129ed8bc074b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c696b322c5b34b94a6c2d8880a6b66f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b45429a13cb047a681b876c33e4f58a6",
              "IPY_MODEL_77b061c305d441d0bc1caf49e4e0216a",
              "IPY_MODEL_e8f808e677ae43049fe3f0d078d50b4a"
            ],
            "layout": "IPY_MODEL_5b7949c7feb94a7088293772e73c899b"
          }
        },
        "b45429a13cb047a681b876c33e4f58a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01566330fca842ce9c347d284a870242",
            "placeholder": "​",
            "style": "IPY_MODEL_23daa37100434f25af5a2450cd553c81",
            "value": "100%"
          }
        },
        "77b061c305d441d0bc1caf49e4e0216a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badf53d5820a461faf5acd8788a770af",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc928330eca241b69718f1590d57f37e",
            "value": 6
          }
        },
        "e8f808e677ae43049fe3f0d078d50b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e4d87949f2342c181c749ddb4643ad3",
            "placeholder": "​",
            "style": "IPY_MODEL_9d671c55e33c4ac29ee49616b8345cdc",
            "value": " 6/6 [00:00&lt;00:00, 257.94it/s]"
          }
        },
        "5b7949c7feb94a7088293772e73c899b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01566330fca842ce9c347d284a870242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23daa37100434f25af5a2450cd553c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "badf53d5820a461faf5acd8788a770af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc928330eca241b69718f1590d57f37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e4d87949f2342c181c749ddb4643ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d671c55e33c4ac29ee49616b8345cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}