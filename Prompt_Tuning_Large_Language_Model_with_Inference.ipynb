{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishabhg97/Misc/blob/main/Prompt_Tuning_Large_Language_Model_with_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This colab file is created by [Pragnakalp Techlabs](https://pragnakalp.com/) using [Nvidia Nemo](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Multitask_Prompt_and_PTuning.ipynb).\n",
        "\n",
        "You can copy this colab in your drive and then execute the command in given order. For more details check our [blog](https://www.pragnakalp.com/prompt-tuning-for-large-language-models-with-inference/)."
      ],
      "metadata": {
        "id": "ZctgsYE5MkoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This colab file is to prompt tune GPT 1.3B for Sentiment Analysis task.**"
      ],
      "metadata": {
        "id": "kJ7zOobXA9F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NeMo Installation**\n",
        "\n",
        "Setup notebook:\n",
        "\n",
        "\n",
        "1.   Open a new Python 3 notebook.\n",
        "\n",
        "2.   Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "\n",
        "3. Run the cell below to set up dependencie\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Gormg0BykQyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BRANCH='main'"
      ],
      "metadata": {
        "id": "CFx6SaYKlisP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhO25x_QDru1",
        "outputId": "da18229e-9b09-4e56-bbb7-6c5ee5592d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,235 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,143 kB]\n",
            "Fetched 3,659 kB in 6s (622 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libsndfile1 is already the newest version (1.0.28-4ubuntu0.18.04.2).\n",
            "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 20 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get update && apt-get install -y libsndfile1 ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7iF9LvCDr_a",
        "outputId": "5975f3a0-f70a-4e99-ce06-bb20927a6636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.8/dist-packages (0.29.32)\n"
          ]
        }
      ],
      "source": [
        "!pip install Cython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning==1.7.7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muvz581P3YVB",
        "outputId": "68b1ffdf-ad91-4cde-8ab4-bbef8d9229b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning==1.7.7 in /usr/local/lib/python3.8/dist-packages (1.7.7)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (4.64.1)\n",
            "Requirement already satisfied: pyDeprecate>=0.3.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (0.3.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (2.9.1)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (0.10.3)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (2022.11.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (1.21.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (4.4.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.8/dist-packages (from pytorch-lightning==1.7.7) (1.13.0+cu116)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (3.8.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.23.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (4.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (6.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch-lightning==1.7.7) (3.0.9)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.8.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.19.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (59.5.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.3.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.15.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.51.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.7) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch-lightning==1.7.7) (2.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx==1.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ij7mWI63cAZ",
        "outputId": "836118e2-035f-44bc-c8e0-602ad1ae4c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: onnx==1.7.0 in /usr/local/lib/python3.8/dist-packages (1.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (3.19.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from onnx==1.7.0) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo-toolkit==1.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e86oTP12zIg",
        "outputId": "774edb8b-a0f9-4db7-a228-ef9e0a3521e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nemo-toolkit==1.12.0 in /usr/local/lib/python3.8/dist-packages (1.12.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (4.64.1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.56.4)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (3.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.3.6)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.21.6)\n",
            "Requirement already satisfied: ruamel.yaml in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.17.21)\n",
            "Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (59.5.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.13.0+cu116)\n",
            "Requirement already satisfied: onnx>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.7.0)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (0.11.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (2.8.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (1.14.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.8/dist-packages (from nemo-toolkit==1.12.0) (2.3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (4.4.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnx>=1.7.0->nemo-toolkit==1.12.0) (3.19.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (3.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->nemo-toolkit==1.12.0) (5.4.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->nemo-toolkit==1.12.0) (3.0.9)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->nemo-toolkit==1.12.0) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->nemo-toolkit==1.12.0) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->nemo-toolkit==1.12.0) (3.11.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->nemo-toolkit==1.12.0) (3.0.4)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /usr/local/lib/python3.8/dist-packages (from ruamel.yaml->nemo-toolkit==1.12.0) (0.2.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->nemo-toolkit==1.12.0) (1.7.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqWofZ5BDwzx",
        "outputId": "31c929ff-7c6e-4948-9bb7-62174afb6bfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 10648, done.\u001b[K\n",
            "remote: Counting objects: 100% (125/125), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 10648 (delta 67), reused 59 (delta 33), pack-reused 10523\u001b[K\n",
            "Receiving objects: 100% (10648/10648), 15.15 MiB | 16.25 MiB/s, done.\n",
            "Resolving deltas: 100% (7330/7330), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ericharper/apex.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBeHoclhEjw9",
        "outputId": "72759775-2bbb-49a5-d05c-72eba3e48d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/apex\n"
          ]
        }
      ],
      "source": [
        "cd apex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qofuUJCAElLK",
        "outputId": "448d32ea-767d-43ec-85db-98d4ea2310fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: checking out 'nm_v1.11.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at e8e5761 comment check\n"
          ]
        }
      ],
      "source": [
        "!git checkout nm_v1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_o9NL7OEmbp",
        "outputId": "58e5cb71-5911-4418-df55-a50efd4ca444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py:232: UserWarning: Disabling all use of wheels due to the use of --build-option / --global-option / --install-option.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Using pip 21.1.3 from /usr/local/lib/python3.8/dist-packages/pip (python 3.8)\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Non-user install because site-packages writeable\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-hibvuvhl\n",
            "Created temporary directory: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Initialized build tracking at /tmp/pip-req-tracker-ajk_2xwp\n",
            "Created build tracker: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Entered build tracker: /tmp/pip-req-tracker-ajk_2xwp\n",
            "Created temporary directory: /tmp/pip-install-2sni3oow\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-vmvyo5j3\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-ajk_2xwp'\n",
            "    Running setup.py (path:/tmp/pip-req-build-vmvyo5j3/setup.py) egg_info for package from file:///content/apex\n",
            "    Created temporary directory: /tmp/pip-pip-egg-info-n5k_3v97\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.13.0+cu116\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info\n",
            "    writing /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file '/tmp/pip-pip-egg-info-n5k_3v97/apex.egg-info/SOURCES.txt'\n",
            "  Source in /tmp/pip-req-build-vmvyo5j3 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-ajk_2xwp'\n",
            "Created temporary directory: /tmp/pip-unpack-u_3qixbh\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/lib/python3.8/dist-packages\n",
            "  sysconfig: /usr/lib/python3.8/site-packages\n",
            "  Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/include/python3.8/apex\n",
            "  sysconfig: /usr/include/python3.8/apex\n",
            "  Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local/bin\n",
            "  sysconfig: /usr/bin\n",
            "  Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "  distutils: /usr/local\n",
            "  sysconfig: /usr\n",
            "  Additional context:\n",
            "  user = False\n",
            "  home = None\n",
            "  root = None\n",
            "  prefix = None\n",
            "  Created temporary directory: /tmp/pip-record-ntu444yn\n",
            "    Running command /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-vmvyo5j3/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-vmvyo5j3/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext --fast_layer_norm --distributed_adam --deprecated_fused_adam install --record /tmp/pip-record-ntu444yn/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.8/apex\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.13.0+cu116\n",
            "\n",
            "\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2021 NVIDIA Corporation\n",
            "    Built on Sun_Feb_14_21:12:58_PST_2021\n",
            "    Cuda compilation tools, release 11.2, V11.2.152\n",
            "    Build cuda_11.2.r11.2/compiler.29618528_0\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.8\n",
            "    creating build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/_autocast_utils.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.8/apex\n",
            "    creating build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.8/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.8/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/parallel_state.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/enums.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/log_util.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    copying apex/transformer/microbatches.py -> build/lib.linux-x86_64-3.8/apex/transformer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_mixed_precision_lamb.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.8/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/__init__.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    copying apex/fused_dense/fused_dense.py -> build/lib.linux-x86_64-3.8/apex/fused_dense\n",
            "    creating build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.8/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.8/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.8/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.8/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.8/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    copying apex/transformer/functional/fused_softmax.py -> build/lib.linux-x86_64-3.8/apex/transformer/functional\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/_batchsampler.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    copying apex/transformer/_data/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/_data\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    copying apex/transformer/layers/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/transformer/layers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/mappings.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/cross_entropy.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/random.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/layers.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/memory.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    copying apex/transformer/tensor_parallel/data.py -> build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/grad_scaler.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    copying apex/transformer/amp/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/amp\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/utils.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/_timers.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    copying apex/transformer/pipeline_parallel/p2p_communication.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_gpt.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/commons.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/global_vars.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_bert.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/distributed_test_base.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/standalone_transformer_lm.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    copying apex/transformer/testing/arguments.py -> build/lib.linux-x86_64-3.8/apex/transformer/testing\n",
            "    creating build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/__init__.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/common.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    copying apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/clip_grad.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    copying apex/contrib/clip_grad/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/clip_grad\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/conv_bias_relu.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    copying apex/contrib/conv_bias_relu/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.8/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_memory.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    copying apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> build/lib.linux-x86_64-3.8/apex/contrib/peer_memory\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    copying apex/contrib/focal_loss/focal_loss.py -> build/lib.linux-x86_64-3.8/apex/contrib/focal_loss\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/index_mul_2d.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    copying apex/contrib/index_mul_2d/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/permutation_lib.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    copying apex/contrib/fmha/fmha.py -> build/lib.linux-x86_64-3.8/apex/contrib/fmha\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/bottleneck_module_test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/halo_exchangers.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    copying apex/contrib/bottleneck/test.py -> build/lib.linux-x86_64-3.8/apex/contrib/bottleneck\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    copying apex/contrib/layer_norm/layer_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/layer_norm\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.8/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/transducer.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    copying apex/contrib/transducer/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/transducer\n",
            "    creating build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/__init__.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels\n",
            "    creating build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.8/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:476: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/utils/cpp_extension.py:387: UserWarning: The detected CUDA version (11.2) has a minor version mismatch with the version that was used to compile PyTorch (11.6). Most likely this shouldn't be a problem.\n",
            "      warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.8\n",
            "    creating build/temp.linux-x86_64-3.8/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/include/python3.8 -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/flatten_unflatten.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'distributed_adam_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.8/apex\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=distributed_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=distributed_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/multi_tensor_distopt_adam_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/distributed_adam_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_kernel_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_l2norm_scale_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/multi_tensor_lamb_mp.cu -o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_kernel_mp.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_l2norm_scale_kernel.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb.o build/temp.linux-x86_64-3.8/csrc/multi_tensor_lamb_mp.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.8/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/welford.cu -o build/temp.linux-x86_64-3.8/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/syncbn.o build/temp.linux-x86_64-3.8/csrc/welford.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> layer_norm(at::Tensor, at::IntArrayRef, double):\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:157:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> layer_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:178:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:179:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:180:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> layer_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:202:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double):\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:244:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:245:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:246:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:247:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:270:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:271:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(mean);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:272:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:273:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:274:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:145:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:145:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:147:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:275:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(beta);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> rms_norm(at::Tensor, at::IntArrayRef, double):\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:313:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> rms_norm_affine(at::Tensor, at::IntArrayRef, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:332:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:333:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> rms_norm_affine_mixed_dtypes(at::Tensor, at::IntArrayRef, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:353:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function at::Tensor rms_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, double):\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:390:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:391:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:392:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function std::vector<at::Tensor> rms_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::IntArrayRef, at::Tensor, double):\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:413:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(dout);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:414:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:415:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(input);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:301:42: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/macros/Macros.h:200:64: note: in definition of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY(expr) (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                    ^~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:464:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {            \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:301:23: note: in expansion of macro TORCH_CHECK\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:303:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:416:3: note: in expansion of macro CHECK_INPUT\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.8/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.8/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>):\n",
            "    csrc/mlp.cpp:57:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:86: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({static_cast<long>(reserved_size)}, inputs[0].type());\n",
            "                                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:67:59: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, inputs[0].type());\n",
            "                                                               ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:69:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:72:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:69:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>):\n",
            "    csrc/mlp.cpp:115:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:121:67: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:124:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:126:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:130:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:138:99: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({static_cast<long>(work_size / sizeof(scalar_t))}, inputs[0].type());\n",
            "                                                                                                       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:140:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:124:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/mlp.o build/temp.linux-x86_64-3.8/csrc/mlp_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_dense_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/fused_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    csrc/fused_dense.cpp: In function at::Tensor linear_bias_forward(at::Tensor, at::Tensor, at::Tensor):\n",
            "    csrc/fused_dense.cpp:30:63: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                   ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:33:55: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:35:50: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:37:15: warning: unused variable b_ptr [-Wunused-variable]\n",
            "         scalar_t* b_ptr = bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:38:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:35:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_bias_backward(at::Tensor, at::Tensor, at::Tensor):\n",
            "    csrc/fused_dense.cpp:64:69: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight = at::empty({out_features, in_features}, input.type());\n",
            "                                                                         ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:68:54: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias = at::empty({out_features}, input.type());\n",
            "                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:70:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:73:55: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:75:50: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:77:15: warning: unused variable d_b_ptr [-Wunused-variable]\n",
            "         scalar_t* d_b_ptr = d_bias.data_ptr<scalar_t>();\n",
            "                   ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp:78:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_bias_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:75:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_forward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "    csrc/fused_dense.cpp:106:70: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:107:70: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto gelu_in = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                          ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:108:67: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto output2 = at::empty({batch_size, out_features}, input.type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:111:55: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:113:50: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:118:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_forward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:113:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_gelu_linear_forward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In function std::vector<at::Tensor> linear_gelu_linear_backward(at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor, at::Tensor):\n",
            "    csrc/fused_dense.cpp:149:73: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight1 = at::empty({hidden_features, in_features}, input.type());\n",
            "                                                                             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:150:74: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_weight2 = at::empty({out_features, hidden_features}, input.type());\n",
            "                                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:151:58: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias1 = at::empty({hidden_features}, input.type());\n",
            "                                                              ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:152:55: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_bias2 = at::empty({out_features}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:153:66: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_input = at::empty({batch_size, in_features}, input.type());\n",
            "                                                                      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:154:72: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto d_output1 = at::empty({batch_size, hidden_features}, input.type());\n",
            "                                                                            ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/fused_dense.cpp:157:55: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto lt_workspace = at::empty({1 << 22}, input.type());\n",
            "                                                           ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:159:50: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "                                                      ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:238:28: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/ATen.h:11:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:4,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:9,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/fused_dense.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:241:56: warning: c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&) is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:268:3: note: in expansion of macro AT_DISPATCH_SWITCH\n",
            "       AT_DISPATCH_SWITCH(                                        \\\n",
            "       ^~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:132:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:263:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Double, __VA_ARGS__) \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:264:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Float, __VA_ARGS__)  \\\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    csrc/fused_dense.cpp: In lambda function:\n",
            "    csrc/fused_dense.cpp:162:10: warning: unused variable result [-Wunused-variable]\n",
            "         auto result = linear_gelu_linear_backward_cuda<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:244:7: note: in definition of macro AT_DISPATCH_SWITCH\n",
            "           __VA_ARGS__                                                           \\\n",
            "           ^~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:97:3: note: in expansion of macro AT_PRIVATE_CASE_TYPE_USING_HINT\n",
            "       AT_PRIVATE_CASE_TYPE_USING_HINT(enum_type, scalar_t, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:265:3: note: in expansion of macro AT_DISPATCH_CASE\n",
            "       AT_DISPATCH_CASE(at::ScalarType::Half, __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Dispatch.h:269:19: note: in expansion of macro AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF\n",
            "           TYPE, NAME, AT_DISPATCH_CASE_FLOATING_TYPES_AND_HALF(__VA_ARGS__))\n",
            "                       ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
            "    csrc/fused_dense.cpp:159:3: note: in expansion of macro AT_DISPATCH_FLOATING_TYPES_AND_HALF\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(input.type(), \"linear_bias_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/fused_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_dense_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    csrc/fused_dense_cuda.cu(1148): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1272): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1273): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1274): warning: variable \"status\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1329): warning: variable \"alpha\" was declared but never referenced\n",
            "\n",
            "    csrc/fused_dense_cuda.cu(1330): warning: variable \"beta_zero\" was declared but never referenced\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/fused_dense.o build/temp.linux-x86_64-3.8/csrc/fused_dense_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_upper_triang_masked_softmax_cuda' extension\n",
            "    creating build/temp.linux-x86_64-3.8/csrc/megatron\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_upper_triang_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_upper_triang_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_upper_triang_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'generic_scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/generic_scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=generic_scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/generic_scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'scaled_masked_softmax_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/scaled_masked_softmax_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=scaled_masked_softmax_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax.o build/temp.linux-x86_64-3.8/csrc/megatron/scaled_masked_softmax_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_weight_gradient_mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense.cpp -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.cu -o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_86,code=sm_86 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_weight_gradient_mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_cuda.o build/temp.linux-x86_64-3.8/csrc/megatron/fused_weight_gradient_dense_16bit_prec_cuda.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fused_adam_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/fused_adam_cuda.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function void strided_check_finite(at::Tensor&, at::Tensor&, int, int):\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:26:2: note: in expansion of macro CHECK_INPUT\n",
            "      CHECK_INPUT(p_copy);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function void adam(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float):\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:30:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:31:33: note: in expansion of macro CHECK_INPUT\n",
            "             if (p_copy.numel() > 0) CHECK_INPUT(p_copy);\n",
            "                                     ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:32:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:33:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:34:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function void reversible_adam(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float):\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:44:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:45:33: note: in expansion of macro CHECK_INPUT\n",
            "             if (p_copy.numel() > 0) CHECK_INPUT(p_copy);\n",
            "                                     ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:46:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:47:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:48:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function void maybe_adam_undo(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, float, float, float, float, int, int, int, float):\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:58:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(p);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:59:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(m);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:60:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(v);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:61:9: note: in expansion of macro CHECK_INPUT\n",
            "             CHECK_INPUT(g);\n",
            "             ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp: In function void maybe_cast(at::Tensor&, at::Tensor&, at::Tensor&):\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:70:2: note: in expansion of macro CHECK_INPUT\n",
            "      CHECK_INPUT(p_in);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/c10/core/Device.h:5:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:11,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:41: warning: at::DeprecatedTypeProperties& at::Tensor::type() const is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                             ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:264:39: note: in definition of macro C10_EXPAND_MSVC_WORKAROUND\n",
            "     #define C10_EXPAND_MSVC_WORKAROUND(x) x\n",
            "                                           ^\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:284:34: note: in expansion of macro C10_UNLIKELY\n",
            "     #define C10_UNLIKELY_OR_CONST(e) C10_UNLIKELY(e)\n",
            "                                      ^~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:336:7: note: in expansion of macro C10_UNLIKELY_OR_CONST\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                                          \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/c10/util/Exception.h:631:32: note: in expansion of macro TORCH_INTERNAL_ASSERT\n",
            "         C10_EXPAND_MSVC_WORKAROUND(TORCH_INTERNAL_ASSERT(cond, __VA_ARGS__)); \\\n",
            "                                    ^~~~~~~~~~~~~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:15:23: note: in expansion of macro AT_ASSERTM\n",
            "     #define CHECK_CUDA(x) AT_ASSERTM(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:17:24: note: in expansion of macro CHECK_CUDA\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:71:2: note: in expansion of macro CHECK_INPUT\n",
            "      CHECK_INPUT(p_out);\n",
            "      ^\n",
            "    In file included from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/ATen/Tensor.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
            "                     from /usr/local/lib/python3.8/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from apex/contrib/csrc/optimizers/fused_adam_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.8/dist-packages/torch/include/ATen/core/TensorBody.h:216:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/csrc -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fused_adam_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/optimizers/fused_adam_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fused_adam_cuda.cpython-38-x86_64-linux-gnu.so\n",
            "    building 'fast_layer_norm' extension\n",
            "    creating build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm\n",
            "    x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_api.cpp -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_api.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -I./apex/contrib/csrc/layer_norm/ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/tmp/pip-req-build-vmvyo5j3/apex/contrib/csrc/layer_norm -I/usr/local/lib/python3.8/dist-packages/torch/include -I/usr/local/lib/python3.8/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.8/dist-packages/torch/include/TH -I/usr/local/lib/python3.8/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.8 -c apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.cu -o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -gencode arch=compute_70,code=sm_70 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_BFLOAT16_OPERATORS__ -U__CUDA_NO_BFLOAT16_CONVERSIONS__ -U__CUDA_NO_BFLOAT162_OPERATORS__ -U__CUDA_NO_BFLOAT162_CONVERSIONS__ -I./apex/contrib/csrc/layer_norm/ --expt-relaxed-constexpr --expt-extended-lambda --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -gencode arch=compute_80,code=sm_80 --threads 4 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -DTORCH_EXTENSION_NAME=fast_layer_norm -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_api.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_fwd_cuda_kernel.o build/temp.linux-x86_64-3.8/apex/contrib/csrc/layer_norm/ln_bwd_semi_cuda_kernel.o -L/usr/local/lib/python3.8/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda_cu -ltorch_cuda_cpp -o build/lib.linux-x86_64-3.8/fast_layer_norm.cpython-38-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_layer_norm_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_dense_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/amp_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fused_adam_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/LARC.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/multiproc.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/distributed.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.8/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.8/apex/_autocast_utils.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/functional/fused_softmax.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/functional\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/_batchsampler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/_data/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/_data\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/parallel_state.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/enums.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/layers/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/layers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/mappings.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/cross_entropy.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/random.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/layers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/memory.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/tensor_parallel/data.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/log_util.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/microbatches.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/grad_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/common.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/_timers.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/pipeline_parallel/p2p_communication.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_gpt.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/commons.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/global_vars.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_bert.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/distributed_test_base.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/standalone_transformer_lm.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    copying build/lib.linux-x86_64-3.8/apex/transformer/testing/arguments.py -> /usr/local/lib/python3.8/dist-packages/apex/transformer/testing\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_mixed_precision_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.8/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fused_dense/fused_dense.py -> /usr/local/lib/python3.8/dist-packages/apex/fused_dense\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/models.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/cells.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.8/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.8/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.8/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/clip_grad.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/clip_grad/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/conv_bias_relu.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/conv_bias_relu/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchanger_1d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_memory.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/focal_loss/focal_loss.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/index_mul_2d.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/index_mul_2d/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/sparsity/permutation_lib.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/fmha/fmha.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/bottleneck_module_test.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/halo_exchangers.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/bottleneck/test.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/layer_norm/layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/transducer.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    copying build/lib.linux-x86_64-3.8/apex/contrib/transducer/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/mlp.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/mlp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.8/apex/normalization/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/amp.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/opt.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.8/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__init__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/scaler.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/wrap.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/utils.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/frontend.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/handle.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_initialize.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/__version__.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/rnn_compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/compat.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/apex/amp/_amp_state.py -> /usr/local/lib/python3.8/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.8/fused_weight_gradient_mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/fast_layer_norm.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/scaled_upper_triang_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/apex_C.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/syncbn.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/generic_scaled_masked_softmax_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/mlp_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.8/distributed_adam_cuda.cpython-38-x86_64-linux-gnu.so -> /usr/local/lib/python3.8/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/LARC.py to LARC.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/distributed.py to distributed.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/_autocast_utils.py to _autocast_utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/functional/fused_softmax.py to fused_softmax.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/_batchsampler.py to _batchsampler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/_data/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/parallel_state.py to parallel_state.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/enums.py to enums.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/layers/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/mappings.py to mappings.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/cross_entropy.py to cross_entropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/random.py to random.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/layers.py to layers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/memory.py to memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/tensor_parallel/data.py to data.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/log_util.py to log_util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/microbatches.py to microbatches.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/grad_scaler.py to grad_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_with_interleaving.py to fwd_bwd_pipelining_with_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_pipelining_without_interleaving.py to fwd_bwd_pipelining_without_interleaving.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/common.py to common.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/schedules/fwd_bwd_no_pipelining.py to fwd_bwd_no_pipelining.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/_timers.py to _timers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/p2p_communication.py to p2p_communication.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_gpt.py to standalone_gpt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/commons.py to commons.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/global_vars.py to global_vars.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_bert.py to standalone_bert.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/distributed_test_base.py to distributed_test_base.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/standalone_transformer_lm.py to standalone_transformer_lm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/transformer/testing/arguments.py to arguments.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_mixed_precision_lamb.py to fused_mixed_precision_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fused_dense/fused_dense.py to fused_dense.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/models.py to models.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/cells.py to cells.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/clip_grad.py to clip_grad.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/clip_grad/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/conv_bias_relu.py to conv_bias_relu.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/conv_bias_relu/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_halo_exchanger_1d.py to peer_halo_exchanger_1d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_memory.py to peer_memory.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/peer_memory/peer_halo_exchange_module_tests.py to peer_halo_exchange_module_tests.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/focal_loss/focal_loss.py to focal_loss.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/index_mul_2d.py to index_mul_2d.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/index_mul_2d/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/exhaustive_search.py to exhaustive_search.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/permutation_utilities.py to permutation_utilities.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/call_permutation_search_kernels.py to call_permutation_search_kernels.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_search_kernels/channel_swap.py to channel_swap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/sparsity/permutation_lib.py to permutation_lib.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/fmha/fmha.py to fmha.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/bottleneck.py to bottleneck.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/bottleneck_module_test.py to bottleneck_module_test.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/halo_exchangers.py to halo_exchangers.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/bottleneck/test.py to test.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/layer_norm/layer_norm.py to layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/transducer.py to transducer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/contrib/transducer/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py to mlp.cpython-38.pyc\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:41: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      if activation is 'none':\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:43: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      elif activation is 'relu':\n",
            "    /usr/local/lib/python3.8/dist-packages/apex/mlp/mlp.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "      elif activation is 'sigmoid':\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/mlp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/normalization/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/amp.py to amp.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/opt.py to opt.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__init__.py to __init__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/scaler.py to scaler.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/wrap.py to wrap.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/utils.py to utils.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/frontend.py to frontend.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/handle.py to handle.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_initialize.py to _initialize.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/__version__.py to __version__.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/compat.py to compat.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-38.pyc\n",
            "    byte-compiling /usr/local/lib/python3.8/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-38.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    adding license file 'LICENSE'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.8/dist-packages/apex-0.1-py3.8.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-ntu444yn/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "Value for scheme.platlib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.purelib does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/lib/python3.8/dist-packages\n",
            "sysconfig: /usr/lib/python3.8/site-packages\n",
            "Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/include/python3.8/UNKNOWN\n",
            "sysconfig: /usr/include/python3.8/UNKNOWN\n",
            "Value for scheme.scripts does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local/bin\n",
            "sysconfig: /usr/bin\n",
            "Value for scheme.data does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
            "distutils: /usr/local\n",
            "sysconfig: /usr\n",
            "Additional context:\n",
            "user = False\n",
            "home = None\n",
            "root = None\n",
            "prefix = None\n",
            "Successfully installed apex-0.1\n",
            "Removed build tracker: '/tmp/pip-req-tracker-ajk_2xwp'\n"
          ]
        }
      ],
      "source": [
        "!pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" --global-option=\"--fast_layer_norm\" --global-option=\"--distributed_adam\" --global-option=\"--deprecated_fused_adam\" ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RaZWSi6EoIz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wget \n",
        "import pathlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xet61W4kXHJU",
        "outputId": "4de75504-e5f2-42ae-a9bf-6f9478485419"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXzqgUXlI_Rg"
      },
      "outputs": [],
      "source": [
        "# You can replace DATA_DIR and NEMO_DIR with your own locations\n",
        "DATA_DIR = \"data\"\n",
        "NEMO_DIR = '.'\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GqRWR-g_I_Ow",
        "outputId": "3c19e341-c844-43ad-8c31-33b7917651f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./prompt_learning_financial_phrase_bank_preprocessing.py'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# download the preprocessing scripts from GitHub\n",
        "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/scripts/dataset_processing/nlp/financial_phrase_bank/prompt_learning_financial_phrase_bank_preprocessing.py', NEMO_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While downloading the below dataset, if you face \"ERROR 403: Forbidden.\" Error, then copy the dataset link and paste in your browser, it will automatically download the zip file. Then you can upload it the colab."
      ],
      "metadata": {
        "id": "k0Mm8txXPAhj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noh20ZCjI_L3",
        "outputId": "ed5bf6d3-9646-4dc0-ce69-cbec126f3ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-30 13:30:44--  https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip\n",
            "Resolving www.researchgate.net (www.researchgate.net)... 104.17.32.105, 104.17.33.105, 2606:4700::6811:2169, ...\n",
            "Connecting to www.researchgate.net (www.researchgate.net)|104.17.32.105|:443... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2022-12-30 13:30:44 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILYVf56oI-_3",
        "outputId": "eb10074c-9168-483e-ad9a-1ed2633bc065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  FinancialPhraseBank-v1.0.zip\n",
            "   creating: data/FinancialPhraseBank-v1.0/\n",
            "  inflating: data/FinancialPhraseBank-v1.0/License.txt  \n",
            "   creating: data/__MACOSX/\n",
            "   creating: data/__MACOSX/FinancialPhraseBank-v1.0/\n",
            "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._License.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/README.txt  \n",
            "  inflating: data/__MACOSX/FinancialPhraseBank-v1.0/._README.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_50Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_66Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_75Agree.txt  \n",
            "  inflating: data/FinancialPhraseBank-v1.0/Sentences_AllAgree.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip FinancialPhraseBank-v1.0.zip -d {DATA_DIR}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud_4yDqtI-3g",
        "outputId": "d15a056d-3bcb-4559-b9d5-52a16e20fa1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .@neutral\r\n",
            "For the last quarter of 2010 , Componenta 's net sales doubled to EUR131m from EUR76m for the same period a year earlier , while it moved to a zero pre-tax profit from a pre-tax loss of EUR7m .@positive\r\n",
            "In the third quarter of 2010 , net sales increased by 5.2 % to EUR 205.5 mn , and operating profit by 34.9 % to EUR 23.5 mn .@positive\r\n",
            "Operating profit rose to EUR 13.1 mn from EUR 8.7 mn in the corresponding period in 2007 representing 7.7 % of net sales .@positive\r\n"
          ]
        }
      ],
      "source": [
        "# What the financial phrase bank dataset looks like before processing\n",
        "SENTIMENT_DIR = os.path.join(DATA_DIR, \"FinancialPhraseBank-v1.0\")\n",
        "!head -4 $SENTIMENT_DIR/Sentences_AllAgree.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwusuUc7Jw0S",
        "outputId": "aa9319f5-c1ae-4692-b92d-95aa9f295875"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl\n",
            "\r  0% 0/1811 [00:00<?, ?it/s]\r100% 1811/1811 [00:00<00:00, 233618.89it/s]\n",
            "Saving val split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
            "\r  0% 0/226 [00:00<?, ?it/s]\r100% 226/226 [00:00<00:00, 208883.36it/s]\n",
            "Saving test split to data/FinancialPhraseBank-v1.0/financial_phrase_bank_test.jsonl\n",
            "\r  0% 0/227 [00:00<?, ?it/s]\r100% 227/227 [00:00<00:00, 248009.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Preprocess financial phrase bank dataset\n",
        "!python $NEMO_DIR/prompt_learning_financial_phrase_bank_preprocessing.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJmMldCJyGj",
        "outputId": "b7c4656a-9d9d-4abf-ee6b-586fba82a8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"taskname\": \"sentiment\", \"sentence\": \"The new organization consists of two business units : Charging & Messaging and Finance & Administration .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Danske Bank is Denmark 's largest bank with 3.5 million customers .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total value of the deliveries is some EUR65m .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"In future , the plant will focus on the production of flange profiles for wind farm towers .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company said that 80 % of the shares of the holding company will be sold to Meadville Holdings Limited , a Hong Kong listed parent company of the Meadville Group .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"With this appointment Kaupthing Bank aims to further co-ordinate Capital Markets activities within the Group and to improve the overall service to clients .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total value of these two contracts is over EUR 21 million .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company 's board of directors will propose a dividend of EUR 0.95 per share for 2008 at the annual general meeting , scheduled to be held on March 23 , 2009 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finlan 's listed food industry company HKScan Group controlled companies in the Baltics improved revenues by EUR 3.5 mn to EUR 160.4 mn in 2010 from EUR 156.9 mn in the year before .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Due to the rapid decrease in net sales , personnel reductions have been carried out on a wider scale than initially expected .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The actions are expected to deliver annual cost savings of some EUR15-20m .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Operating loss totaled EUR 0.3 mn compared to a profit of EUR 2.2 mn in the corresponding period in 2007 .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the six-month period increased , respectively from EUR0 .4 m and EUR3 .2 m , as compared to the corresponding period in 2005 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Therefore , the company 's 2005 result will remain weaker than that of 2004 .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Operating profit margin increased from 11.2 % to 11.7 % .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This new partnership agreement represents a significant milestone for both parties .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"According to Finnish petrol station chain St1 's managing director Kim Wiio , the company was forced to make purchases with rising prices in the first half of 2008 , and now consumer prices are going down almost daily due to competition .\", \"label\": \" negative\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The center will be built in the Kapuli district of Mantsala beside the Hanko-Mantsala-Porvoo road near the new direct rail link between Lahti and Jarvenpaa .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The machine will have an annual production capacity of 200,000 tonnes of super-calendered magazine paper and other paper grades based on recovered fiber , Stora Enso said .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Supported Nokia phones include : N96 , N95-8GB , N95 , N93-N931 , N92 , N85 , N82 , N81 , N80 , N79 , N78 , N77 , N76 , N75 , N73 , N72 , N71 , E90 , E71 , E70 , E66 , E65 , E62 , E61-E61i , E60 , E51 , E50 , Touch Xpress 5800 , 6220 Classic , 6210 Navigator , 6120 Classic , 6110 Navigator , 5700 , 5500 , 5320XM .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Earnings per share EPS amounted to EUR0 .01 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Diluted earnings per share ( EPS ) rose to EUR 1.05 from EUR 0.64 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finnish financial group Aktia 's operating profit for 2009 increased to EUR 47.0 mn from EUR 6.6 mn in 2008 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Metso 's delivery will include a complete coated board line with related air systems and two winders .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Water Treatment Products In Australia Today , Global Research & Data Services is going to publish a market analysis about the market for chemical water treatment products in Australia .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the six-month period increased , respectively , from EUR13 .8 m and EUR143 .6 m , as compared to the corresponding period in 2007 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Both operating profit and net sales for the 12-month period increased , respectively from EUR20 .8 m and EUR177 .7 m , as compared to the financial year 2004 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Diluted earnings per share ( EPS ) rose to EUR 3.68 from EUR 0.50 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Other details were not provided .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"From Merisatama to the far corners of the world Asfaltti Osakeyhti+\\u00c2 Lemmink+\\u00f1inen was established in 1910 by a group of master builders in Helsinki as a specialist business and subcontractor .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"25 November 2010 - Finnish paints and coatings company Tikkurila Oyj ( HEL : TIK1V ) said today that Finnish state-owned investment company Solidium Oy sold its 14.7 % stake in the company for a total of EUR98m .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"He wore a black beanie-type cap and a black jacket .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Finnish silicon wafer technology company Okmetic Oyj OMX Helsinki : OKM1V reported on Thursday 30 October an operating profit of EUR7 .4 m for January-September 2008 , up from EUR6 .1 m in the corresponding period in 2007 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"He joins Technopolis from KONE where he has held various positions within the Group , most recently as Director of Service Business and Business Development for KONE s Middle Eastern operations .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Depending on the market situation , such projects are sold after 1 to 3 years after completion .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The company 's plant in Russia will continue to make tyres for its near markets , while the plant in Nokia in Finland will manufacture tyres for other markets .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"After 1 April 2007 Cencorp will not have any own employees in the territory .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Commenting on the deal , Shane Lennon , SVP of Marketing & Product Development at GyPSii said : ?\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The tests , conducted at Nokia Siemens ' LTE center of competence in Espoo , Finland , follow the company 's production start of LTE-ready Flexi Multiradio Base Stations for the 800 MHz band in April 2010 , and complement earlier tests with Nokia on the 2100 MHz and 2600 MHz bands .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This could be any of us at any time , '' she said .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Profit after taxes for the period was up to EUR0 .9 m , from EUR0 .01 m last year .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Short-term licenses for the games cost as little as $ 3 while purchasing a game outright can cost as much as $ 10 or $ 15 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The total number of voting rights is 74,612,523 .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"This amount will not be included in the pensionable salary .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The operating profit for Grain Trading increased to EUR 2.0 mn from EUR 1.4 mn in 2005 .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"They are responsible for their own operations , customer relationships , and the development of these .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Consolidated net sales increased 16 % to reach EUR74 .8 m , while operating profit amounted to EUR0 .9 m compared to a loss of EUR0 .7 m in the prior year period .\", \"label\": \" positive\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"Okmetic Board of Directors has also decided on a new share ownership program directed to the company 's top management .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"The size of a cider bottle will remain unchanged .\", \"label\": \" neutral\"}\n",
            "{\"taskname\": \"sentiment\", \"sentence\": \"With this , the company will exit the contract manufacturing service segment .\", \"label\": \" neutral\"}\n"
          ]
        }
      ],
      "source": [
        "# What the financial phrase bank dataset looks like after processing\n",
        "!head -50 $SENTIMENT_DIR/financial_phrase_bank_train.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxP7SxtuJzL9"
      },
      "outputs": [],
      "source": [
        "from omegaconf import OmegaConf\n",
        "\n",
        "CONFIG_DIR = os.path.join(NEMO_DIR, \"conf\")\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "\n",
        "# Download the example config file\n",
        "wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/language_modeling/conf/megatron_gpt_prompt_learning_config.yaml', CONFIG_DIR)\n",
        "\n",
        "# Load the example config file so we can start editing it\n",
        "CONFIG_PATH = os.path.join(CONFIG_DIR, \"megatron_gpt_prompt_learning_config.yaml\")\n",
        "config = OmegaConf.load(CONFIG_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d99kn6SFJ1Np"
      },
      "outputs": [],
      "source": [
        "config.model.data.train_ds = [f\"{SENTIMENT_DIR}/financial_phrase_bank_train.jsonl\"]\n",
        "config.model.data.validation_ds = [f\"{SENTIMENT_DIR}/financial_phrase_bank_val.jsonl\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt Formatting**\n",
        "\n"
      ],
      "metadata": {
        "id": "gM1WVrSsm8zl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Vh1ymNJ2hV"
      },
      "outputs": [],
      "source": [
        "config.model.task_templates = [\n",
        "    {\n",
        "        \"taskname\": \"sentiment\",\n",
        "        \"prompt_template\": \"<|VIRTUAL_PROMPT_0|> {sentence} sentiment:{label}\",\n",
        "        \"total_virtual_tokens\": 10,\n",
        "        \"virtual_token_splits\": [10],\n",
        "        \"truncate_field\": None,\n",
        "        \"answer_only_loss\": True,\n",
        "        \"answer_field\": \"label\",\n",
        "    },\n",
        "  \n",
        "    \n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each task_template item has 5 fields.\n",
        "\n",
        "\n",
        "*  **prompt_template** is a string showing the model where to place virtual tokens and how to map dataset json fields to where they belong in the model prompt.\n",
        "*  **taskname** refers to the same taskname in the dataset json objects.\n",
        "*  **total_virtual_tokens** specifies the total number of virtual tokens that will be inserted into the model prompt.\n",
        "*  **virtual_token_splits** specifies the number of virtual tokens that belong at each <|VIRTUAL_PROMPT_#|> marker. virtual_token_splits values should add up to total_virtual_tokens. The number of virtual_token_splits should match the number of <|VIRTUAL_PROMPT_#|> markers.\n",
        "* **truncate_field** specifies which field in the data json to truncate if the length of the input exceeds the maximum sequence length of the model. If truncate_field is set to None, examples that are too long are simply dropped from the dataset.\n",
        "* **answer_only_loss** Whether to limit loss calculation to only the answer portion of the prompt during tuning. True Strongly recommended for long prompts, but shorter prompts with single word answers seem to benefit from setting this to False.\n",
        "* **answer_field** The field in the data json corresponding to the answer. The loss will only be calculated on this portion of the prompt if answer_only_loss is True. The answer field must be at the end of the prompt template.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KzI-D7mfnTWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Tasks**"
      ],
      "metadata": {
        "id": "u02DpDjdsQUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuQnn9yCJ301"
      },
      "outputs": [],
      "source": [
        "config.model.existing_tasks = []\n",
        "config.model.new_tasks = [\"sentiment\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting The Pre-Trained GPT Model**"
      ],
      "metadata": {
        "id": "-Kmo_DHBslcV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We still need to set which GPT model we want to prompt tune. We need .nemo version of LLM model. We have tested neno-megatron-gpt-345m and nemo-megatron-gpt-1.3B on Google Colab Pro.\n",
        "You can download .nemo file from the below link:\n",
        "\n",
        "1. neno-megatron-gpt-345m: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/megatron_gpt_345m.nemo\n",
        "2. nemo-megatron-gpt-1.3B: https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo\n",
        "3. nemo-megatron-mt5-3B: https://huggingface.co/nvidia/nemo-megatron-mt5-3B/resolve/main/nemo_megatron_mt5_3b_bf16_tp1.nemo\n",
        "4. nemo-megatron-t5-3B: https://huggingface.co/nvidia/nemo-megatron-t5-3B/resolve/main/nemo_megatron_t5_3b_bf16_tp1.nemo\n",
        "5. nemo-megatron-gpt-5B: https://huggingface.co/nvidia/nemo-megatron-gpt-5B/resolve/main/nemo_gpt5B_fp16_tp1.nemo\n",
        "6. nemo-megatron-gpt-20B: https://huggingface.co/nvidia/nemo-megatron-gpt-20B/resolve/main/nemo_gpt20B_bf16_tp4.nemo\n",
        "\n"
      ],
      "metadata": {
        "id": "zJ7W85clsyHd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Idk1bz-J5TK",
        "outputId": "5bc2dd59-2796-4e39-902e-5afe8630a4fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PretrainedModelInfo(\n",
              " \tpretrained_model_name=megatron_gpt_345m,\n",
              " \tdescription=345M parameter GPT generative Megatron model.,\n",
              " \tlocation=https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/megatron_gpt_345m.nemo\n",
              " )]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Check the GPT .nemo models that are available\n",
        "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
        "MegatronGPTModel.list_available_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we are using nemo_gpt1.3B_fp16.nemo large model for prompt tuning. You can download the nemo_gpt1.3B_fp16.nemo as shown in below:\n"
      ],
      "metadata": {
        "id": "V7AQ31XltKLA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJk_P0DZtulv",
        "outputId": "ab8193b4-b2cc-4cb8-e4d6-2959dd424293"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-30 12:56:01--  https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo\n",
            "Resolving huggingface.co (huggingface.co)... 18.235.116.140, 34.238.87.3, 54.144.222.129, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.235.116.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/67/62/676214ea0cb0c2e59b6a0a42b68a8d0dbbc5082d3db7328c793f08fab0c0d577/0e3c4482d75ecdf622c567690d1323b363bf41074e00b8f7d1a43305535557c3?response-content-disposition=attachment%3B%20filename%3D%22nemo_gpt1.3B_fp16.nemo%22&Expires=1672664162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY3LzYyLzY3NjIxNGVhMGNiMGMyZTU5YjZhMGE0MmI2OGE4ZDBkYmJjNTA4MmQzZGI3MzI4Yzc5M2YwOGZhYjBjMGQ1NzcvMGUzYzQ0ODJkNzVlY2RmNjIyYzU2NzY5MGQxMzIzYjM2M2JmNDEwNzRlMDBiOGY3ZDFhNDMzMDU1MzU1NTdjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMm5lbW9fZ3B0MS4zQl9mcDE2Lm5lbW8lMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzI2NjQxNjJ9fX1dfQ__&Signature=lO7gAiCRXQysiMkUCk2MA1W2kTTf12Sdjmh5WKOvj0tD~CZrD2q7DEI1~eAjlc4hdQOAl5HwsABf~qRgMX2uYtQRgTw6ZddBCeQrUICcus6hcXLUFW-DR5KolvA3uNk5bsYNmGKNVEe5G4ojIRmJQ59p1H62lsm5-rvkRAsF58Khxgb0gMYbJ9UdFJjVRrxCWW3OJpbkwj7MUtWDZSItBnfAK69gpYSAuSDVCoIakrBc0UWeKV6ARAH0X2Bj61NIJHj5vXMoHJSHvIYU1FU9eosB8J5NTDhUwUpfGdl9NoNDVXAt794jMCCz9Fm8YdNEipL6WXyyy~I3OYvGP3200Q__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2022-12-30 12:56:02--  https://cdn-lfs.huggingface.co/repos/67/62/676214ea0cb0c2e59b6a0a42b68a8d0dbbc5082d3db7328c793f08fab0c0d577/0e3c4482d75ecdf622c567690d1323b363bf41074e00b8f7d1a43305535557c3?response-content-disposition=attachment%3B%20filename%3D%22nemo_gpt1.3B_fp16.nemo%22&Expires=1672664162&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzY3LzYyLzY3NjIxNGVhMGNiMGMyZTU5YjZhMGE0MmI2OGE4ZDBkYmJjNTA4MmQzZGI3MzI4Yzc5M2YwOGZhYjBjMGQ1NzcvMGUzYzQ0ODJkNzVlY2RmNjIyYzU2NzY5MGQxMzIzYjM2M2JmNDEwNzRlMDBiOGY3ZDFhNDMzMDU1MzU1NTdjMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPWF0dGFjaG1lbnQlM0IlMjBmaWxlbmFtZSUzRCUyMm5lbW9fZ3B0MS4zQl9mcDE2Lm5lbW8lMjIiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2NzI2NjQxNjJ9fX1dfQ__&Signature=lO7gAiCRXQysiMkUCk2MA1W2kTTf12Sdjmh5WKOvj0tD~CZrD2q7DEI1~eAjlc4hdQOAl5HwsABf~qRgMX2uYtQRgTw6ZddBCeQrUICcus6hcXLUFW-DR5KolvA3uNk5bsYNmGKNVEe5G4ojIRmJQ59p1H62lsm5-rvkRAsF58Khxgb0gMYbJ9UdFJjVRrxCWW3OJpbkwj7MUtWDZSItBnfAK69gpYSAuSDVCoIakrBc0UWeKV6ARAH0X2Bj61NIJHj5vXMoHJSHvIYU1FU9eosB8J5NTDhUwUpfGdl9NoNDVXAt794jMCCz9Fm8YdNEipL6WXyyy~I3OYvGP3200Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.35.24.38, 13.35.24.76, 13.35.24.128, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.35.24.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3017949806 (2.8G) [application/x-gzip]\n",
            "Saving to: nemo_gpt1.3B_fp16.nemo\n",
            "\n",
            "nemo_gpt1.3B_fp16.n 100%[===================>]   2.81G  21.4MB/s    in 2m 25s  \n",
            "\n",
            "2022-12-30 12:58:28 (19.8 MB/s) - nemo_gpt1.3B_fp16.nemo saved [3017949806/3017949806]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/nvidia/nemo-megatron-gpt-1.3B/resolve/main/nemo_gpt1.3B_fp16.nemo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ydXg5TEUJ6e"
      },
      "outputs": [],
      "source": [
        "# Set GPT model path on prompt learning config\n",
        "gpt_file_name = \"nemo_gpt1.3B_fp16.nemo\"\n",
        "\n",
        "config.model.language_model_path = gpt_file_name"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also set where we want the final prompt tuned model to be saved by setting model.nemo_path"
      ],
      "metadata": {
        "id": "WaQxGGI7tjSD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIOFOrMuUfAp"
      },
      "outputs": [],
      "source": [
        "config.model.nemo_path = \"prompt_tuned_model.nemo\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setting Prompt-tuning Hyperparameters**"
      ],
      "metadata": {
        "id": "GsS8Fseyt0a2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mdjsLJEUhUT"
      },
      "outputs": [],
      "source": [
        "from nemo.collections.nlp.modules.common import VirtualPromptStyle\n",
        "config.model.virtual_prompt_style = VirtualPromptStyle.P_TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeujNl9hUksm"
      },
      "outputs": [],
      "source": [
        "config.model.p_tuning.dropout = 0.0\n",
        "config.model.p_tuning.num_layers = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsPhaMunUl-L",
        "outputId": "41379178-f029-4695-e1c3-d30fe1b44218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed: 1234\n",
            "nemo_path: prompt_tuned_model.nemo\n",
            "virtual_prompt_style: P_TUNING\n",
            "tensor_model_parallel_size: 1\n",
            "pipeline_model_parallel_size: 1\n",
            "global_batch_size: 8\n",
            "micro_batch_size: 4\n",
            "restore_path: null\n",
            "language_model_path: nemo_gpt1.3B_fp16.nemo\n",
            "save_nemo_on_validation_end: true\n",
            "existing_tasks: []\n",
            "new_tasks:\n",
            "- sentiment\n",
            "sequence_parallel: false\n",
            "activations_checkpoint_granularity: null\n",
            "activations_checkpoint_method: null\n",
            "activations_checkpoint_num_layers: null\n",
            "task_templates:\n",
            "- taskname: sentiment\n",
            "  prompt_template: <|VIRTUAL_PROMPT_0|> {sentence} sentiment:{label}\n",
            "  total_virtual_tokens: 10\n",
            "  virtual_token_splits:\n",
            "  - 10\n",
            "  truncate_field: null\n",
            "  answer_only_loss: true\n",
            "  answer_field: label\n",
            "prompt_tuning:\n",
            "  new_prompt_init_methods:\n",
            "  - text\n",
            "  new_prompt_init_text:\n",
            "  - some init text goes here\n",
            "p_tuning:\n",
            "  encoder_type: tpmlp\n",
            "  dropout: 0.0\n",
            "  num_layers: 2\n",
            "  encoder_hidden: 2048\n",
            "  init_std: 0.023\n",
            "data:\n",
            "  train_ds:\n",
            "  - data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl\n",
            "  validation_ds:\n",
            "  - data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
            "  add_eos: true\n",
            "  shuffle: true\n",
            "  num_workers: 8\n",
            "  pin_memory: true\n",
            "  train_cache_data_path: null\n",
            "  validation_cache_data_path: null\n",
            "  test_cache_data_path: null\n",
            "  load_cache: false\n",
            "optim:\n",
            "  name: fused_adam\n",
            "  lr: 0.0001\n",
            "  weight_decay: 0.01\n",
            "  betas:\n",
            "  - 0.9\n",
            "  - 0.98\n",
            "  sched:\n",
            "    name: CosineAnnealing\n",
            "    warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    monitor: val_loss\n",
            "    reduce_on_plateau: false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Final model config\n",
        "print(OmegaConf.to_yaml(config.model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
        "from pytorch_lightning.plugins.environments.torchelastic_environment import TorchElasticEnvironment\n",
        "\n",
        "# lets modify some trainer configs\n",
        "# checks if we have GPU available and uses it\n",
        "accelerator = 'gpu' if torch.cuda.is_available() else 'cpu'\n",
        "config.trainer.accelerator = accelerator\n",
        "config.trainer.devices = 1\n",
        "config.trainer.max_epochs = 1\n",
        "config.trainer.val_check_interval = 1.0\n",
        "\n",
        "# for PyTorch Native AMP set precision=16\n",
        "config.trainer.precision = 16 if torch.cuda.is_available() else 32\n",
        "\n",
        "# setup cluster environment parameters\"\n",
        "# use torch elastic cluster environment so `create_process_externally` is True\n",
        "# the launcher is set to None. It will not try to spawn new processes.\n",
        "# It won't create the misconfiguration error because of the `interactive session`\n",
        "os.environ[\"LOCAL_RANK\"] = '0'\n",
        "os.environ[\"RANK\"] = '0'\n",
        "os.environ[\"WORLD_SIZE\"] = '1'\n",
        "\n",
        "strategy = NLPDDPStrategy(find_unused_parameters=False,no_ddp_communication_hook=True)\n",
        "plugins = [TorchElasticEnvironment()]\n",
        "trainer = pl.Trainer(plugins= plugins, strategy=strategy, **config.trainer)\n",
        "\n",
        "print(\"Trainer config - \\n\")\n",
        "print(OmegaConf.to_yaml(config.trainer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BImtciFEHdDY",
        "outputId": "86f2f31b-8074-42f5-bb9f-cebf16649599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit native Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainer config - \n",
            "\n",
            "devices: 1\n",
            "accelerator: gpu\n",
            "num_nodes: 1\n",
            "precision: 16\n",
            "logger: false\n",
            "enable_checkpointing: false\n",
            "replace_sampler_ddp: false\n",
            "max_epochs: 1\n",
            "max_steps: -1\n",
            "log_every_n_steps: 10\n",
            "val_check_interval: 1.0\n",
            "gradient_clip_val: 1.0\n",
            "resume_from_checkpoint: null\n",
            "benchmark: false\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the PyTorch Lightning Trainer**\n",
        "\n",
        "NeMo models are primarily PyTorch Lightning modules and therefore are entirely compatible with the PyTorch Lightning ecosystem.\n",
        "\n"
      ],
      "metadata": {
        "id": "JS_kA8o9uGMv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JuxqSWIUr3T",
        "outputId": "c7fe8a7e-cd44-49c5-a8d3-72850d22826d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:04:36 exp_manager:291] Experiments will be logged at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36\n",
            "[NeMo I 2022-12-30 13:04:36 exp_manager:669] TensorboardLogger has been set up\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:04:36 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/trainer.py:2274: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
            "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
            "    \n",
            "[NeMo W 2022-12-30 13:04:36 exp_manager:919] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to -1. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
            "[NeMo W 2022-12-30 13:04:36 exp_manager:711] Found save_best_model is True and save_nemo_on_train_end is False. Set save_nemo_on_train_end to True to automatically save the best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36\n"
          ]
        }
      ],
      "source": [
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "# Set name of the experiment \n",
        "config.name = 'sentiment_intent_slot_p_tuning'\n",
        "config.exp_manager.resume_if_exists = False\n",
        "\n",
        "# Init the experiment manager and view the exp_dir\n",
        "exp_dir = exp_manager(trainer, config.get(\"exp_manager\", None))\n",
        "exp_dir = str(exp_dir)\n",
        "print(exp_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NafqBUES_pI8"
      },
      "outputs": [],
      "source": [
        "# Set some of the learning parameters\n",
        "config.model.optim.lr = 1e-4\n",
        "config.model.precision = config.trainer.precision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompt tuning the model**"
      ],
      "metadata": {
        "id": "ox4Nn-0tuXh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830,
          "referenced_widgets": [
            "a3ddcac41b3f45fd8c672c4a06cf8f6a",
            "45f2234be5a24dfda8105b35e2331e94",
            "e1aa8279090b4006a3c229051a9f2244",
            "aa591eabf7d64d0f98817f21913cc1c3",
            "2d49abefe02d4d53b0260c2bdad000d4",
            "2dc4235ffff44a7faf52fa2fa9440d2b",
            "187b5d5f17884ca4a3efac58e940b1f2",
            "feae736aa3244c62a4555fb69799144d",
            "f048188eb15b44e4ac140258663e323d",
            "2945f9cd0f6e448682ad5f07897e176d",
            "b846bf1ab1f4426f9ffcfebd9b1f242b",
            "1d471c8bbbcf42c1b1ca554a0a24fc86",
            "92a5a4be717b4032af288b27d87bd9e4",
            "dbe2ef4d36174f3992fba258da4e6d30",
            "61060153d32b44c2a176cc441ccbeb21",
            "6c922e862dfc4036b72d5c13d9b23029",
            "11661e5286534824829d5fc813fe0a45",
            "05aae9da1460429cb448313640ff0d42",
            "69abe9f38ad64cadb49987f62392c46d",
            "0ad90c95e2ed4f0cbf93980cb7c2365d",
            "f62758144c1b4fbab5e21da3a525e502",
            "0e785d6eee214829a9667195561e2710",
            "054719fe01ea4983a4c7cbddb92c2981",
            "5dc2cd106e764deb92b5faef195fd780",
            "ca0e66b9346f45b1a7d8bf46b324688e",
            "e59b089725b24432b434fa56e878f757",
            "9c2987e6ca854926b76c7f485aa44667",
            "e106a86ec2154506970285b173d43a3d",
            "afc83f6900394874961788341488bc04",
            "41935e7ea0384045b20153b5c1423550",
            "a145948cd7374d04a9c4150da16fbdf0",
            "f1e41b1e8e9e42298a9ff7a1eddb3034",
            "e4e87afbb3f649deaa1b7ad74d33253d"
          ]
        },
        "id": "tY5gwHDi_sci",
        "outputId": "c9e47816-92d9-4082-e34b-f76406df9490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:04:43 megatron_init:204] Rank 0 has data parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:207] All data parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:208] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:216] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:217] All model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:227] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:231] All tensor model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:232] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:246] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:258] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:264] All pipeline model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:265] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:266] All embedding group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:04:43 megatron_init:267] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:204] Rank 0 has data parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:207] All data parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:208] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:216] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:217] All model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:227] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:231] All tensor model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:232] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:246] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:258] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:264] All pipeline model parallel group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:265] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:266] All embedding group ranks: [[0]]\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_init:267] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2022-12-30 13:05:33 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: None, and merges file: None\n",
            "[NeMo I 2022-12-30 13:05:33 megatron_utils:210] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\n",
            "[NeMo I 2022-12-30 13:05:35 megatron_utils:210] Downloading from https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\n",
            "[NeMo I 2022-12-30 13:05:36 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /root/.cache/torch/megatron/megatron-gpt-345m_vocab, merges_files: /root/.cache/torch/megatron/megatron-gpt-345m_merges, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3ddcac41b3f45fd8c672c4a06cf8f6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.json:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d471c8bbbcf42c1b1ca554a0a24fc86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "054719fe01ea4983a4c7cbddb92c2981"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using sep_token, but it is not set yet.\n",
            "Using cls_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:47 megatron_base_model:185] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
            "[NeMo I 2022-12-30 13:05:56 nlp_overrides:370] Model MegatronGPTModel was successfully restored from /content/nemo_gpt1.3B_fp16.nemo.\n",
            "[NeMo I 2022-12-30 13:05:56 auto_tokenizer:171] 10 special tokens added, resize your model accordingly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using pad_token, but it is not set yet.\n",
            "Using mask_token, but it is not set yet.\n"
          ]
        }
      ],
      "source": [
        "from nemo.collections.nlp.models.language_modeling.megatron_gpt_prompt_learning_model import MegatronGPTPromptLearningModel\n",
        "\n",
        "model = MegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "53f28eb73b3143b4a3c0c9b7af245e9f",
            "0be4d0a77db1456fb4c6fe94f9097ad7",
            "4726f18878b5475899fd06af3c7451af",
            "a8b3401e4f73452cae980693d10fb50c",
            "19e87a1d8c9c48d88cdd5b740146820c",
            "d1a9c2420a024a539e5c986162865ef6",
            "10f8372bd3cc46d1bd8babdf62d59567",
            "08c7bfcedf9f4f95beae78aa9664c4a4",
            "8b9f366c4d0d4b848e8e63b05508dcbe",
            "0035b4f69e6f4c67897e3da1887eebf8",
            "e8863d42e37647b3801a4d87430d7fa6",
            "0492adb32ca14825881ae78b08659a47",
            "e341623bd3a74bbab561619e6185caf4",
            "e521cdd468044fab9d1fcbda0f8fe847",
            "c34df705810446369b16ffed4494622c",
            "e9d256ad0c424b8ebf8a2e202a56c4a6",
            "2f71ed466e6d43c995bf3b713493b746",
            "47093af54e7b40a088bdcbf3108a6e99",
            "1cff8f66eb164b86b401b12f525afe7f",
            "b42f45ab97b2482f88e5c4a66bc8271a",
            "bf73af868d2f40b59ecf0d589bffdd1d",
            "405251b84b7b4c0c8606ba66c1c00905",
            "77b9cebb5a2f4929b16906e6d1ed3418",
            "9f68e146bb05481190f8e56b9349cc1d",
            "0f5493e840054091b563bcb896ce41b5",
            "f1a6d147e98b4e46ac8daf643885f79e",
            "b1c08923025140569f5338aa7a67381d",
            "14312ed950f04098b68c1d7d75ee5bfc",
            "4d20140def2f4b98918a0d8d89cc739f",
            "a91ec5c5731e413ebcdb57197ac28ae7",
            "7b4979acc63e447fb7bd91144ecec1cb",
            "d01392c7100545f2ba20d4b5185b46f6",
            "1b0502209376485c97c2d144f5a4c1ee",
            "13e121ae5afa4c24bbe3528290ee71a8",
            "814fb06fe14d4c0cbf286cc28b13a305",
            "ff21b332f262448b91d952e4f6098029",
            "9a4bd3af142d4c309d59949bfb29657a",
            "51bbcc37f3d940e9aeeabe83e6a52aa6",
            "b85f0ca3ac6f41c0803f7e8ee06ef448",
            "ff35ece87fe344a28220cc55683ce6ac",
            "519f0ddf92854081bfa9a7d43f2b31c9",
            "0139378a76264587b59a1d12f8342adb",
            "42ab197933cd4de2a8b06fb5f8b2ed84",
            "cc8b306f107548a5964bde9f715db563",
            "c22e6059a4ce4c81a853d10b67dd799c",
            "50ce7a1c97fb4911aa6fff99dbaf7135",
            "135e5372c07a4a63ad83cfdc01a88cad",
            "bc782152f196411791fe1e926318c5ec",
            "3b97147b48dd4382be9b0bb41e2dbb0b",
            "93a24dc5f6174d59a8d882bf3e7f7e17",
            "b741c965c286486086ed2029e3a91d74",
            "692d8999486c42babb7f0fea3beb7025",
            "9cc9e7f9e3f04fcd9f8c0455d8329d04",
            "333f3643f70d4d55b7d16918f2227feb",
            "ef289c7641454ea399129ed8bc074b08"
          ]
        },
        "id": "nHmvxin3_suv",
        "outputId": "1f26e8b1-bbdf-4aeb-ca0e-6581dc06a719"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/configuration_validator.py:220: LightningDeprecationWarning: The `LightningModule.on_pretrain_routine_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `LightningModule.on_fit_start` instead.\n",
            "      rank_zero_deprecation(\n",
            "    \n",
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py:37: UserWarning: MASTER_ADDR environment variable is not defined. Set as localhost\n",
            "      rank_zero_warn(\"MASTER_ADDR environment variable is not defined. Set as localhost\")\n",
            "    \n",
            "[NeMo W 2022-12-30 13:05:56 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/plugins/environments/torchelastic_environment.py:45: UserWarning: MASTER_PORT environment variable is not defined. Set as 12910\n",
            "      rank_zero_warn(\"MASTER_PORT environment variable is not defined. Set as 12910\")\n",
            "    \n",
            "INFO:pytorch_lightning.utilities.distributed:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "INFO:pytorch_lightning.utilities.rank_zero:----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:56 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53f28eb73b3143b4a3c0c9b7af245e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:05:57 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "      warnings.warn(_create_warning_msg(\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0492adb32ca14825881ae78b08659a47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:57 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:05:58 nlp_overrides:103] Configuring DDP for model parallelism.\n",
            "[NeMo I 2022-12-30 13:05:58 modelPT:597] Optimizer config = FusedAdam (\n",
            "    Parameter Group 0\n",
            "        betas: [0.9, 0.98]\n",
            "        bias_correction: True\n",
            "        eps: 1e-08\n",
            "        lr: 0.0001\n",
            "        weight_decay: 0.01\n",
            "    )\n",
            "[NeMo I 2022-12-30 13:05:58 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fb74c39f220>\" \n",
            "    will be used during training (effective maximum steps = 226) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    max_steps: 226\n",
            "    )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name            | Type                   | Params\n",
            "-----------------------------------------------------------\n",
            "0 | frozen_model    | MegatronGPTModel       | 812 M \n",
            "1 | word_embeddings | VocabParallelEmbedding | 103 M \n",
            "2 | prompt_table    | PromptTable            | 0     \n",
            "3 | prompt_encoder  | PromptEncoderMLP       | 8.4 M \n",
            "-----------------------------------------------------------\n",
            "8.4 M     Trainable params\n",
            "812 M     Non-trainable params\n",
            "820 M     Total params\n",
            "1,641.587 Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Sanity Checking: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77b9cebb5a2f4929b16906e6d1ed3418"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:03 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:535: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "      warning_cache.warn(\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13e121ae5afa4c24bbe3528290ee71a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:231: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
            "      warning_cache.warn(\n",
            "    \n",
            "[NeMo W 2022-12-30 13:06:05 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c22e6059a4ce4c81a853d10b67dd799c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Epoch 0, global step 226: 'val_loss' reached 0.06929 (best 0.06929), saving model to '/content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt' as top 2\n",
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Restored all states from the checkpoint file at /content/nemo_experiments/sentiment_intent_slot_p_tuning/2022-12-30_13-04-36/checkpoints/megatron_gpt_prompt_tune--val_loss=0.069-step=226.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:55 megatron_gpt_prompt_learning_model:683] All p-tuned prompts where moved to the prompt table.\n",
            "[NeMo I 2022-12-30 13:06:55 megatron_gpt_prompt_learning_model:696] The final model was saved to prompt_tuned_model.nemo\n"
          ]
        }
      ],
      "source": [
        "# Training set to 10 epochs by default in a cell above\n",
        "trainer.fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Inference after Prompt-tuning**"
      ],
      "metadata": {
        "id": "-57ykzIuuf08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtoDPK5nGbqE"
      },
      "outputs": [],
      "source": [
        "test_examples = [\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The products have a low salt and fat content .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The agreement is valid for four years .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The company is well positioned in Brazil and Uruguay .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\"},\n",
        "    {\"taskname\": \"sentiment\", \"sentence\": \"The movie was not good.\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375,
          "referenced_widgets": [
            "c696b322c5b34b94a6c2d8880a6b66f7",
            "b45429a13cb047a681b876c33e4f58a6",
            "77b061c305d441d0bc1caf49e4e0216a",
            "e8f808e677ae43049fe3f0d078d50b4a",
            "5b7949c7feb94a7088293772e73c899b",
            "01566330fca842ce9c347d284a870242",
            "23daa37100434f25af5a2450cd553c81",
            "badf53d5820a461faf5acd8788a770af",
            "dc928330eca241b69718f1590d57f37e",
            "0e4d87949f2342c181c749ddb4643ad3",
            "9d671c55e33c4ac29ee49616b8345cdc"
          ]
        },
        "id": "6w145640MKTb",
        "outputId": "683936a7-9a50-4a2b-a3f1-cd3c180b8402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:56 gpt_prompt_learning_dataset:85] Loading and tokenizing dataset ... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c696b322c5b34b94a6c2d8880a6b66f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2022-12-30 13:06:56 gpt_prompt_learning_dataset:192] Skipped 0 sentences, sequence length too short or too long even after truncation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2022-12-30 13:06:57 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
            "      warnings.warn(\"This function is only for unittest\")\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction results of some sample queries with the trained model:\n",
            "The products have a low salt and fat content . sentiment: neutral\n",
            "------------------------------\n",
            "The agreement is valid for four years . sentiment: neutral\n",
            "------------------------------\n",
            "Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
            "------------------------------\n",
            "The company is well positioned in Brazil and Uruguay . sentiment: neutral\n",
            "------------------------------\n",
            "Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
            "------------------------------\n",
            "The movie was not good. sentiment: negative\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "response = model.generate(inputs=test_examples, length_params=None)\n",
        "\n",
        "\n",
        "print('The prediction results of some sample queries with the trained model:')\n",
        "for result in response['sentences']:\n",
        "    print(result)\n",
        "    print(\"-\" * 30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3ddcac41b3f45fd8c672c4a06cf8f6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_45f2234be5a24dfda8105b35e2331e94",
              "IPY_MODEL_e1aa8279090b4006a3c229051a9f2244",
              "IPY_MODEL_aa591eabf7d64d0f98817f21913cc1c3"
            ],
            "layout": "IPY_MODEL_2d49abefe02d4d53b0260c2bdad000d4"
          }
        },
        "45f2234be5a24dfda8105b35e2331e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2dc4235ffff44a7faf52fa2fa9440d2b",
            "placeholder": "",
            "style": "IPY_MODEL_187b5d5f17884ca4a3efac58e940b1f2",
            "value": "Downloading config.json: 100%"
          }
        },
        "e1aa8279090b4006a3c229051a9f2244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feae736aa3244c62a4555fb69799144d",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f048188eb15b44e4ac140258663e323d",
            "value": 665
          }
        },
        "aa591eabf7d64d0f98817f21913cc1c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2945f9cd0f6e448682ad5f07897e176d",
            "placeholder": "",
            "style": "IPY_MODEL_b846bf1ab1f4426f9ffcfebd9b1f242b",
            "value": " 665/665 [00:00&lt;00:00, 42.1kB/s]"
          }
        },
        "2d49abefe02d4d53b0260c2bdad000d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dc4235ffff44a7faf52fa2fa9440d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "187b5d5f17884ca4a3efac58e940b1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feae736aa3244c62a4555fb69799144d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f048188eb15b44e4ac140258663e323d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2945f9cd0f6e448682ad5f07897e176d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b846bf1ab1f4426f9ffcfebd9b1f242b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d471c8bbbcf42c1b1ca554a0a24fc86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92a5a4be717b4032af288b27d87bd9e4",
              "IPY_MODEL_dbe2ef4d36174f3992fba258da4e6d30",
              "IPY_MODEL_61060153d32b44c2a176cc441ccbeb21"
            ],
            "layout": "IPY_MODEL_6c922e862dfc4036b72d5c13d9b23029"
          }
        },
        "92a5a4be717b4032af288b27d87bd9e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11661e5286534824829d5fc813fe0a45",
            "placeholder": "",
            "style": "IPY_MODEL_05aae9da1460429cb448313640ff0d42",
            "value": "Downloading vocab.json: 100%"
          }
        },
        "dbe2ef4d36174f3992fba258da4e6d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69abe9f38ad64cadb49987f62392c46d",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ad90c95e2ed4f0cbf93980cb7c2365d",
            "value": 1042301
          }
        },
        "61060153d32b44c2a176cc441ccbeb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62758144c1b4fbab5e21da3a525e502",
            "placeholder": "",
            "style": "IPY_MODEL_0e785d6eee214829a9667195561e2710",
            "value": " 0.99M/0.99M [00:00&lt;00:00, 1.04MB/s]"
          }
        },
        "6c922e862dfc4036b72d5c13d9b23029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11661e5286534824829d5fc813fe0a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05aae9da1460429cb448313640ff0d42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69abe9f38ad64cadb49987f62392c46d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ad90c95e2ed4f0cbf93980cb7c2365d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f62758144c1b4fbab5e21da3a525e502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e785d6eee214829a9667195561e2710": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "054719fe01ea4983a4c7cbddb92c2981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5dc2cd106e764deb92b5faef195fd780",
              "IPY_MODEL_ca0e66b9346f45b1a7d8bf46b324688e",
              "IPY_MODEL_e59b089725b24432b434fa56e878f757"
            ],
            "layout": "IPY_MODEL_9c2987e6ca854926b76c7f485aa44667"
          }
        },
        "5dc2cd106e764deb92b5faef195fd780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e106a86ec2154506970285b173d43a3d",
            "placeholder": "",
            "style": "IPY_MODEL_afc83f6900394874961788341488bc04",
            "value": "Downloading merges.txt: 100%"
          }
        },
        "ca0e66b9346f45b1a7d8bf46b324688e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41935e7ea0384045b20153b5c1423550",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a145948cd7374d04a9c4150da16fbdf0",
            "value": 456318
          }
        },
        "e59b089725b24432b434fa56e878f757": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e41b1e8e9e42298a9ff7a1eddb3034",
            "placeholder": "",
            "style": "IPY_MODEL_e4e87afbb3f649deaa1b7ad74d33253d",
            "value": " 446k/446k [00:00&lt;00:00, 500kB/s]"
          }
        },
        "9c2987e6ca854926b76c7f485aa44667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e106a86ec2154506970285b173d43a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc83f6900394874961788341488bc04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41935e7ea0384045b20153b5c1423550": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a145948cd7374d04a9c4150da16fbdf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f1e41b1e8e9e42298a9ff7a1eddb3034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e87afbb3f649deaa1b7ad74d33253d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53f28eb73b3143b4a3c0c9b7af245e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0be4d0a77db1456fb4c6fe94f9097ad7",
              "IPY_MODEL_4726f18878b5475899fd06af3c7451af",
              "IPY_MODEL_a8b3401e4f73452cae980693d10fb50c"
            ],
            "layout": "IPY_MODEL_19e87a1d8c9c48d88cdd5b740146820c"
          }
        },
        "0be4d0a77db1456fb4c6fe94f9097ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1a9c2420a024a539e5c986162865ef6",
            "placeholder": "",
            "style": "IPY_MODEL_10f8372bd3cc46d1bd8babdf62d59567",
            "value": ""
          }
        },
        "4726f18878b5475899fd06af3c7451af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08c7bfcedf9f4f95beae78aa9664c4a4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b9f366c4d0d4b848e8e63b05508dcbe",
            "value": 1
          }
        },
        "a8b3401e4f73452cae980693d10fb50c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0035b4f69e6f4c67897e3da1887eebf8",
            "placeholder": "",
            "style": "IPY_MODEL_e8863d42e37647b3801a4d87430d7fa6",
            "value": " 1811/? [00:01&lt;00:00, 1804.91it/s]"
          }
        },
        "19e87a1d8c9c48d88cdd5b740146820c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1a9c2420a024a539e5c986162865ef6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10f8372bd3cc46d1bd8babdf62d59567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08c7bfcedf9f4f95beae78aa9664c4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8b9f366c4d0d4b848e8e63b05508dcbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0035b4f69e6f4c67897e3da1887eebf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8863d42e37647b3801a4d87430d7fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0492adb32ca14825881ae78b08659a47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e341623bd3a74bbab561619e6185caf4",
              "IPY_MODEL_e521cdd468044fab9d1fcbda0f8fe847",
              "IPY_MODEL_c34df705810446369b16ffed4494622c"
            ],
            "layout": "IPY_MODEL_e9d256ad0c424b8ebf8a2e202a56c4a6"
          }
        },
        "e341623bd3a74bbab561619e6185caf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f71ed466e6d43c995bf3b713493b746",
            "placeholder": "",
            "style": "IPY_MODEL_47093af54e7b40a088bdcbf3108a6e99",
            "value": ""
          }
        },
        "e521cdd468044fab9d1fcbda0f8fe847": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cff8f66eb164b86b401b12f525afe7f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b42f45ab97b2482f88e5c4a66bc8271a",
            "value": 1
          }
        },
        "c34df705810446369b16ffed4494622c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf73af868d2f40b59ecf0d589bffdd1d",
            "placeholder": "",
            "style": "IPY_MODEL_405251b84b7b4c0c8606ba66c1c00905",
            "value": " 226/? [00:00&lt;00:00, 1447.11it/s]"
          }
        },
        "e9d256ad0c424b8ebf8a2e202a56c4a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f71ed466e6d43c995bf3b713493b746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47093af54e7b40a088bdcbf3108a6e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cff8f66eb164b86b401b12f525afe7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b42f45ab97b2482f88e5c4a66bc8271a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf73af868d2f40b59ecf0d589bffdd1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "405251b84b7b4c0c8606ba66c1c00905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77b9cebb5a2f4929b16906e6d1ed3418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9f68e146bb05481190f8e56b9349cc1d",
              "IPY_MODEL_0f5493e840054091b563bcb896ce41b5",
              "IPY_MODEL_f1a6d147e98b4e46ac8daf643885f79e"
            ],
            "layout": "IPY_MODEL_b1c08923025140569f5338aa7a67381d"
          }
        },
        "9f68e146bb05481190f8e56b9349cc1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14312ed950f04098b68c1d7d75ee5bfc",
            "placeholder": "",
            "style": "IPY_MODEL_4d20140def2f4b98918a0d8d89cc739f",
            "value": "Sanity Checking DataLoader 0: 100%"
          }
        },
        "0f5493e840054091b563bcb896ce41b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a91ec5c5731e413ebcdb57197ac28ae7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7b4979acc63e447fb7bd91144ecec1cb",
            "value": 2
          }
        },
        "f1a6d147e98b4e46ac8daf643885f79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d01392c7100545f2ba20d4b5185b46f6",
            "placeholder": "",
            "style": "IPY_MODEL_1b0502209376485c97c2d144f5a4c1ee",
            "value": " 2/2 [00:05&lt;00:00,  2.60s/it]"
          }
        },
        "b1c08923025140569f5338aa7a67381d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "14312ed950f04098b68c1d7d75ee5bfc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d20140def2f4b98918a0d8d89cc739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a91ec5c5731e413ebcdb57197ac28ae7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4979acc63e447fb7bd91144ecec1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d01392c7100545f2ba20d4b5185b46f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0502209376485c97c2d144f5a4c1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13e121ae5afa4c24bbe3528290ee71a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_814fb06fe14d4c0cbf286cc28b13a305",
              "IPY_MODEL_ff21b332f262448b91d952e4f6098029",
              "IPY_MODEL_9a4bd3af142d4c309d59949bfb29657a"
            ],
            "layout": "IPY_MODEL_51bbcc37f3d940e9aeeabe83e6a52aa6"
          }
        },
        "814fb06fe14d4c0cbf286cc28b13a305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85f0ca3ac6f41c0803f7e8ee06ef448",
            "placeholder": "",
            "style": "IPY_MODEL_ff35ece87fe344a28220cc55683ce6ac",
            "value": "Epoch 0: 100%"
          }
        },
        "ff21b332f262448b91d952e4f6098029": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519f0ddf92854081bfa9a7d43f2b31c9",
            "max": 254,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0139378a76264587b59a1d12f8342adb",
            "value": 254
          }
        },
        "9a4bd3af142d4c309d59949bfb29657a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ab197933cd4de2a8b06fb5f8b2ed84",
            "placeholder": "",
            "style": "IPY_MODEL_cc8b306f107548a5964bde9f715db563",
            "value": " 254/254 [00:51&lt;00:00,  4.98it/s, loss=0.151, v_num=4-36, reduced_train_loss=0.0472, global_step=225.0, val_loss=0.0693]"
          }
        },
        "51bbcc37f3d940e9aeeabe83e6a52aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "b85f0ca3ac6f41c0803f7e8ee06ef448": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff35ece87fe344a28220cc55683ce6ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "519f0ddf92854081bfa9a7d43f2b31c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0139378a76264587b59a1d12f8342adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42ab197933cd4de2a8b06fb5f8b2ed84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8b306f107548a5964bde9f715db563": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c22e6059a4ce4c81a853d10b67dd799c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50ce7a1c97fb4911aa6fff99dbaf7135",
              "IPY_MODEL_135e5372c07a4a63ad83cfdc01a88cad",
              "IPY_MODEL_bc782152f196411791fe1e926318c5ec"
            ],
            "layout": "IPY_MODEL_3b97147b48dd4382be9b0bb41e2dbb0b"
          }
        },
        "50ce7a1c97fb4911aa6fff99dbaf7135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93a24dc5f6174d59a8d882bf3e7f7e17",
            "placeholder": "",
            "style": "IPY_MODEL_b741c965c286486086ed2029e3a91d74",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "135e5372c07a4a63ad83cfdc01a88cad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_692d8999486c42babb7f0fea3beb7025",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9cc9e7f9e3f04fcd9f8c0455d8329d04",
            "value": 28
          }
        },
        "bc782152f196411791fe1e926318c5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_333f3643f70d4d55b7d16918f2227feb",
            "placeholder": "",
            "style": "IPY_MODEL_ef289c7641454ea399129ed8bc074b08",
            "value": " 28/28 [00:02&lt;00:00, 10.12it/s]"
          }
        },
        "3b97147b48dd4382be9b0bb41e2dbb0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "93a24dc5f6174d59a8d882bf3e7f7e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b741c965c286486086ed2029e3a91d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "692d8999486c42babb7f0fea3beb7025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9cc9e7f9e3f04fcd9f8c0455d8329d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "333f3643f70d4d55b7d16918f2227feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef289c7641454ea399129ed8bc074b08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c696b322c5b34b94a6c2d8880a6b66f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b45429a13cb047a681b876c33e4f58a6",
              "IPY_MODEL_77b061c305d441d0bc1caf49e4e0216a",
              "IPY_MODEL_e8f808e677ae43049fe3f0d078d50b4a"
            ],
            "layout": "IPY_MODEL_5b7949c7feb94a7088293772e73c899b"
          }
        },
        "b45429a13cb047a681b876c33e4f58a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01566330fca842ce9c347d284a870242",
            "placeholder": "",
            "style": "IPY_MODEL_23daa37100434f25af5a2450cd553c81",
            "value": "100%"
          }
        },
        "77b061c305d441d0bc1caf49e4e0216a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_badf53d5820a461faf5acd8788a770af",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc928330eca241b69718f1590d57f37e",
            "value": 6
          }
        },
        "e8f808e677ae43049fe3f0d078d50b4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e4d87949f2342c181c749ddb4643ad3",
            "placeholder": "",
            "style": "IPY_MODEL_9d671c55e33c4ac29ee49616b8345cdc",
            "value": " 6/6 [00:00&lt;00:00, 257.94it/s]"
          }
        },
        "5b7949c7feb94a7088293772e73c899b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01566330fca842ce9c347d284a870242": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23daa37100434f25af5a2450cd553c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "badf53d5820a461faf5acd8788a770af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc928330eca241b69718f1590d57f37e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0e4d87949f2342c181c749ddb4643ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d671c55e33c4ac29ee49616b8345cdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}